{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs","editUrlLocalized":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/i18n/en/docusaurus-plugin-content-docs/current","isLast":true,"routePriority":-1,"sidebarFilePath":"/Users/hansgun/Documents/code/gits/hansgun.github.io/sidebars.ts","contentPath":"/Users/hansgun/Documents/code/gits/hansgun.github.io/docs","contentPathLocalized":"/Users/hansgun/Documents/code/gits/hansgun.github.io/i18n/en/docusaurus-plugin-content-docs/current","docs":[{"id":"Azure_cloud/AWS_Azure","title":"06. AWS , Azure","description":"- 리소스, 사용자 권한 등을 관리하기 위한 표준 등을 크게 4 가지로 구분할 수 있으며, 각 CSP에서 제공하는 서비스는 다음과 같다.","source":"@site/docs/Azure_cloud/AWS_Azure.md","sourceDirName":"Azure_cloud","slug":"/Azure_cloud/AWS_Azure","permalink":"/docs/Azure_cloud/AWS_Azure","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Azure_cloud/AWS_Azure.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"05. AWS commit message - Azure DevOps Board 연동","permalink":"/docs/Azure_cloud/AWS_codecommit"},"next":{"title":"B. Image_Recorgnition","permalink":"/docs/category/b-image_recorgnition"}},{"id":"Azure_cloud/AWS_codecommit","title":"05. AWS commit message - Azure DevOps Board 연동","description":"- 칸반보드에서 commit history 연동","source":"@site/docs/Azure_cloud/AWS_codecommit.md","sourceDirName":"Azure_cloud","slug":"/Azure_cloud/AWS_codecommit","permalink":"/docs/Azure_cloud/AWS_codecommit","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Azure_cloud/AWS_codecommit.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"04. CI/CD","permalink":"/docs/Azure_cloud/CI_CD"},"next":{"title":"06. AWS , Azure","permalink":"/docs/Azure_cloud/AWS_Azure"}},{"id":"Azure_cloud/CI_CD","title":"04. CI/CD","description":"- (update 중...)","source":"@site/docs/Azure_cloud/CI_CD.md","sourceDirName":"Azure_cloud","slug":"/Azure_cloud/CI_CD","permalink":"/docs/Azure_cloud/CI_CD","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Azure_cloud/CI_CD.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"03. Inner_Architecture","permalink":"/docs/Azure_cloud/Inner_Architecture"},"next":{"title":"05. AWS commit message - Azure DevOps Board 연동","permalink":"/docs/Azure_cloud/AWS_codecommit"}},{"id":"Azure_cloud/Inner_Architecture","title":"03. Inner_Architecture","description":"- cloud native application 개발 간 정리 / 적용한 원칙 및 시스템 구현에 관한 내용 및 체크 사항등을 나열한 문서임","source":"@site/docs/Azure_cloud/Inner_Architecture.md","sourceDirName":"Azure_cloud","slug":"/Azure_cloud/Inner_Architecture","permalink":"/docs/Azure_cloud/Inner_Architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Azure_cloud/Inner_Architecture.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"02. Outer_Architecture","permalink":"/docs/Azure_cloud/Outer_Architecture"},"next":{"title":"04. CI/CD","permalink":"/docs/Azure_cloud/CI_CD"}},{"id":"Azure_cloud/Introduction","title":"01. Introduction","description":"1. 개발 환경 준비","source":"@site/docs/Azure_cloud/Introduction.md","sourceDirName":"Azure_cloud","slug":"/Azure_cloud/Introduction","permalink":"/docs/Azure_cloud/Introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Azure_cloud/Introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"A. Azure_cloud","permalink":"/docs/category/a-azure_cloud"},"next":{"title":"02. Outer_Architecture","permalink":"/docs/Azure_cloud/Outer_Architecture"}},{"id":"Azure_cloud/Outer_Architecture","title":"02. Outer_Architecture","description":"- Hub&Spoke","source":"@site/docs/Azure_cloud/Outer_Architecture.md","sourceDirName":"Azure_cloud","slug":"/Azure_cloud/Outer_Architecture","permalink":"/docs/Azure_cloud/Outer_Architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Azure_cloud/Outer_Architecture.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"01. Introduction","permalink":"/docs/Azure_cloud/Introduction"},"next":{"title":"03. Inner_Architecture","permalink":"/docs/Azure_cloud/Inner_Architecture"}},{"id":"Image_Recorgnition/multiple_object_detecting","title":"01. multiple_object_detecting","description":"가. 개 요","source":"@site/docs/Image_Recorgnition/multiple_object_detecting.md","sourceDirName":"Image_Recorgnition","slug":"/Image_Recorgnition/multiple_object_detecting","permalink":"/docs/Image_Recorgnition/multiple_object_detecting","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Image_Recorgnition/multiple_object_detecting.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"B. Image_Recorgnition","permalink":"/docs/category/b-image_recorgnition"},"next":{"title":"02. object_tracking","permalink":"/docs/Image_Recorgnition/object_tracking"}},{"id":"Image_Recorgnition/object_tracking","title":"02. object_tracking","description":"개체 추적 기술은 개체 감지 기술의 응용버전이다. 개별 감지 결과를 sequential 하게 연결하여 object의 상태 변화를 데이터화 하는 방법이다. 특히 연속된 이미지(이미지를 frame으로 삼는 영상)에서 개체의 위치 정보를 위치의 변동이 없는 배경과 비교한 물리데이터를 추출하여 motion 분석으로 활용하는 기술이 개체 추적이다. 구체적인 task 로는 egomotion, optical flow, tracking 등이 있다. 실제 task의 구분은 모호할 수 있으며, 목적에 따라 여러 기법이 합성된 형태로 적용되기도 한다.","source":"@site/docs/Image_Recorgnition/object_tracking.md","sourceDirName":"Image_Recorgnition","slug":"/Image_Recorgnition/object_tracking","permalink":"/docs/Image_Recorgnition/object_tracking","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Image_Recorgnition/object_tracking.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"01. multiple_object_detecting","permalink":"/docs/Image_Recorgnition/multiple_object_detecting"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"category","label":"A. Azure_cloud","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"Azure_cloud/Introduction"},{"type":"doc","id":"Azure_cloud/Outer_Architecture"},{"type":"doc","id":"Azure_cloud/Inner_Architecture"},{"type":"doc","id":"Azure_cloud/CI_CD"},{"type":"doc","id":"Azure_cloud/AWS_codecommit"},{"type":"doc","id":"Azure_cloud/AWS_Azure"}],"link":{"type":"generated-index","slug":"/category/a-azure_cloud","permalink":"/docs/category/a-azure_cloud"}},{"type":"category","label":"B. Image_Recorgnition","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"Image_Recorgnition/multiple_object_detecting"},{"type":"doc","id":"Image_Recorgnition/object_tracking"}],"link":{"type":"generated-index","slug":"/category/b-image_recorgnition","permalink":"/docs/category/b-image_recorgnition"}}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"All posts","blogPosts":[{"id":"/helllo_world","metadata":{"permalink":"/helllo_world","source":"@site/blog/helllo_world.md","title":"Moved!","description":"Hello !","date":"2024-12-28T12:04:31.000Z","tags":[],"readingTime":0.075,"hasTruncateMarker":true,"authors":[],"frontMatter":{"layout":"post","title":"Moved!","category":"work"},"unlisted":false,"nextItem":{"title":"Setup Prometheus on private k8s cluster","permalink":"/2024/12/18/forth-prometheus"}},"content":"Hello !\n\nJekyll 에서 어제(24.12.28) 이사왔어요!!\n\n<!-- truncate -->"},{"id":"/2024/12/18/forth-prometheus","metadata":{"permalink":"/2024/12/18/forth-prometheus","source":"@site/blog/2024-12-18-forth-prometheus.md","title":"Setup Prometheus on private k8s cluster","description":"summary","date":"2024-12-17T16:16:01.000Z","tags":[{"inline":false,"label":"Observability","permalink":"/tags/Observability","description":"Observability tag description"},{"inline":false,"label":"Prometheus","permalink":"/tags/Prometheus","description":"Prometheus tag description"},{"inline":false,"label":"Loki","permalink":"/tags/Loki","description":"Loki tag description"},{"inline":false,"label":"Tempo","permalink":"/tags/Tempo","description":"Tempo tag description"},{"inline":false,"label":"Opentelemetry","permalink":"/tags/Opentelemetry","description":"Opentelemetry tag description"},{"inline":false,"label":"auto-instrument","permalink":"/tags/auto-instrument","description":"auto-instrument tag description"}],"readingTime":11.37,"hasTruncateMarker":true,"authors":[{"name":"Hanbyul Cho","title":"Engineer","url":"https://github.com/hansgun","page":{"permalink":"/authors/hansgun"},"socials":{"linkedin":"https://www.linkedin.com/in/hanbyulcho1/","github":"https://github.com/hansgun"},"imageURL":"https://github.com/hansgun.png","key":"hansgun"}],"frontMatter":{"layout":"single","title":"Setup Prometheus on private k8s cluster","date":"2024-12-17 16:16:01 -0600","categories":"work","tags":["Observability","Prometheus","Loki","Tempo","Opentelemetry","auto-instrument"],"author_profile":false,"toc":true,"toc_label":"On This Page","toc_icon":"cog","toc_position":"sticky","authors":["hansgun"]},"unlisted":false,"prevItem":{"title":"Moved!","permalink":"/helllo_world"},"nextItem":{"title":"Redis A-S(Active-Standby/Master-Slave) cluster 구축기","permalink":"/2024/11/01/fifth-Redis-cluster"}},"content":"> summary\n<!-- truncate -->\n# Prolog \n- 상용 SaaS Observability 툴은 modern한 그래픽과 편의성을 제공하지만(ex:Datadog), 사용량 base로 과금으로 최적화 하지 않은 Log, Trace 혹은 Profile 기능을 활용 시 과금 폭탄을 맞고 있다. \n- Open Source 이자, k8s 환경에 잘 적용되고, 빠르게 발전중인 Prometheus로 대체 하고자 현 운영중인 private k8s cluster에 prometheus stack을 적용한 이야기를 적어본다. \n- `trace` 기능 구현을 search 하던 중 `open-telemetry` 를 발견하였으며, 다시 구축한다면 `opentelemetry stack` 을 활용하여 scratch 부터 구성하겠지만, `kube-prometheus-stack` 의 편리함을 한 번 맛보고 ~~나니 돌아가고 싶지 않다.~~\n\n## 0. 구성요소 - 설치한 내용\n----\n* Prometheus - [ kube-prometheus-stack ] \n* Loki - for log\n* Tempo, Opentelemetry Agent - for trace\n* Exporters - Redis, PostgreSQL\n\n### 0-1. Trace 설정 관련 \n- 고려한 솔루션은 `Jaeger`, `Zipkin`, `Tempo` 등이 있으며, 앞의 2개 솔루션은 몇 가지 단점이 있으며 사실 상 deprecated 되어 Tempo를 활용하기로 결정 \n  - `Jeager` : CNCF 등록 프로젝트이나, 설정이 복잡함\n  - `Zipkin` : Java 진영의 Trace용 솔루션으로, 커뮤니티 활성화가 되어 있지 않아 이슈 해결의 어려움. \n  - `Tempo` : 상대적으로 신생 프로젝트이나 `Grafana` 진영에서 주 프로젝트로 관리하고, `Grafana` 생태계와 연동하기 좋음. ~~UI! 짱~~\n- `Opentelemetry Auto-instruments`를 이용해 Trace data를 주입하고자 하였으나, `8443 port`를 k8s cluster node 단에서 오픈이 필요하여 이 부분은 생략함. ~~사실 namespace별, service별 operator 배포가 귀찮은 것도..~~\n\n## 1. 환경 설정\n----\n- 필요 프로그램 \n- k8s agent - connecto to cluster \n- helm \n\n### 1-1. 다운로드 환경 \n#### 1-1-1. docker image download\n\n- private cluster 에서 public docker image download가 불가능 하므로, CI/CD Agent 서버에서 이미지 다운로드 후 proxy 서버(Azure Artifacts)에 등록하여 k8s cluster에 설치 함\n- 예제\n\n```bash\n## sharedprdacr.azureacr.io 에 docker login 되어 있어야 함.. azure service account 로 가능 \n## 1. download\ndocker pull docker.io/grafana/grafana:11.2.0 \ndocker pull registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\n\n## 2. taggging\ndocker tag docker.io/grafana/grafana:11.2.0 sharedprdacr.azureacr.io/grafana/grafana:11.2.0\ndocker tag registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6 sharedprdacr.azureacr.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\n\n## 3. push \ndocker push sharedprdacr.azureacr.io/grafana/grafana:11.2.0 \ndocker push sharedprdacr.azureacr.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\n```\n\n## 2. install kube-prometheus-stack  \n----\n### 2-1. helm chart download \n\n- kube-prometheus-stack 설치\n- namespace 명 : `ns-prometheus`\n\n```bash\nWORKDIR = /data/mgmt/prometheus\n\n### repo add\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\n### helm pull\ncd ${WORKDIR}\nhelm pull prometheus-community/kube-prometheus-stack \n\n```\n\n### 2-2. edit values.yaml \n```yaml \ncd kube-prometheus-stack/\n\n## values.yaml 백업\ncp values.yaml values_241217.yaml \n\n### vi 로 values.yaml 파일 편집\n## docker registry 변경\nglobal.imageRegistry : \"sharedacr.azureacr.io\"\nglobal.imagePullSecrets : [XXXXXXXX]\n\n## alert manager off \nalertmanager.enabled : false\n\n## registry\n:s/(docker.io|registry.k8s.io|quay.io|gcr.io|ghcr.io)/shared.azureacr.io/g\n\n\n## grafana : k8s service 로 등록 예정\n\n```\n### 2-3. install prometheus \n```bash\nkubectl config use-context <<cluster context>>; helm upgrade --install prometheus . -n ns-prometheus --create-namespace -f values.yaml\n```\n\n## 3. Log 설정\n----\n- loki를 통하여 log를 전달 받아 cloud에 Object storage 에 저장하는 로직을 구현. \n- Loki에서는 cloud storage 설정, promtail 은 loki와 연결하는 인터페이스 설정이 필요 \n- 설정을 위한 helm chart는 grafana를 이용하므로 개별 설정 변경 전에 먼저 다운 받아 둔다. \n\n```bash\n## helm repo add\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\nhelm pull grafana-stack \n```\n\n### 3-1. Loki 설정\n\n#### 3-1-1. 워킹 디렉터리 변경 \n```bash\ncd ${WORKDIR}/grafana-stack/charts/loki-distributed\n```\n\n#### 3-1-2. `values.yaml` 파일 편집\n```yaml\n...(생략)..\nschemaConfig:\n  configs:\n  - from: \"2024-09-11\"\n    index:\n      period: 24h\n      prefix: index_\n    object_store: azure\n    schema: v13\n    store: tsdb\nstorageConfig:\n  azure:\n    account_name: <스토리지이름> \n    account_key: BO1dfASaldkjfklUXAlkjasldkfjalskjf==\n    container_name: prometheus\n    use_managed_identity: false\n    request_timeout: 0 \n  tsdb_shipper:\n    active_index_directory: var/loki/index ### var 앞에 / 추가할 경우 에러 발생\n    cache_location: var/loki/index_cache\n    cache_ttl: 24h\n  filesystem: \n    directory: var/loki/chunks\n...(하략)...\n```\n\n#### 3-1-3. loki-distributed install\n```bash\n## k8s context 선택\nkubectl config use-context <cluster-context>\n\n## install \nhelm upgrade --install loki . -n ns-prometheus -f values.yaml\n```\n\n#### 3-1-4. Loki 설치 확인 \n\n```bash\n$ k get po -n ns-prometheus | grep -i loki \nNAME                                                        READY   STATUS  RESTART AGE\nloki-loki-distritubed-gateway-6dc5578cb9-j7scf              1/1     Running 0       10d\nloki-loki-distritubed-distributor-6dc5578cb9-j7scf          1/1     Running 0       10d\nloki-loki-distritubed-ingester-0                            1/1     Running 0       10d\nloki-loki-distritubed-querier-0                             1/1     Running 0       10d\nloki-loki-distritubed-query-frontend-8689676f4f-h8wn4-0     1/1     Running 0       10d\n```\n\n### 3-2. promtail 설정\n\n#### 3-2-1. 워킹 디렉터리 변경 \n\n```bash\ncd $WORKDIR/grafana-stack/charts/promtail\n```\n\n#### 3-2-2. `values.yaml` 파일 편집\n```yaml \n....(생략)....\nclients:\n- url: http://loki-loki-distributed-gateway/loki/api/v1/push\n....(하략)....\n```\n\n#### 3-2-3. `install promtail` \n```bash \nhelm upgradee --install promtail . -n ns-prometheus -f values.yaml\n```\n> [to-do] multi-line 처리\n\n\n### 3-3. promtail 설치 확인\n```bash\n$ k get po -n ns-prometheus | grep -i promtail\nNAME                    READY   STATUS  RESTART AGE\npromtail-2f8v9          1/1     Running 0       10d\npromtail-2f8v9          1/1     Running 0       10d\npromtail-2f8v9          1/1     Running 0       10d\npromtail-2f8v9          1/1     Running 0       10d\npromtail-2f8v9          1/1     Running 0       10d\npromtail-2f8v9          1/1     Running 0       10d\n``` \n## 4. `Trace` 설정\n### 4-0. opentelemetry 개요 \n### 4-1. 설치 순서\n- 먼저 opentelemetry 설치 전에 cert manager를 설치필요(requirement)\n- opentelemetry trace는 CRD 형태로 배포 후 operator resource를 설치하고 collector 서비스를 기동하여 수집을 위한 환경을 만든다. \n- 앞서 생성한 collector에 실제 trace 데이터를 전송하는 agent는 몇 가지 방법으로 설치할 수 있는데, CI/CD pipeline을 통해 자동화하려면 크게 2가지 방법으로 정리된다. \n    1. autoinstrument k8s object를 생성하여 label selector를 통하여 자동 적용\n    2. opentelemery agent를 JVM 구동 시 library 형태로 주입하고, 설정에 필요한 환경변수들은 helm 배포 시 주입되도록 조작\n- 앞선 방법 중 1번의 경우 cert manager와 통신을 위해서 k8s cluster nodeport 에 대해 open이 필요하나(`port : 8443`) private cluster의 방화벽 open 필요. ~~정보보안팀에 연락하기 귀찮...~~. 2번 방법으로 진행\n- 결론적으로 설치 순서\n    0. cert manager 설치\n    1. opentelemetry operator \n    2. opentelemetry collector\n    3. opentelemetry instrument\n      - JVM 실행 시 opentelemetry agent library로 실행 주입 \n\n- helm repo add\n```bash\nhelm repo add open-telemetry https://github.com/open-telemetry/opentelemetry-helm-charts\nhelm repo update\nhelm pull \n```\n\n\n### 4-2. `cert manager` 설치\n- k8s 내부 TLS 통신에 대해 인증 처리하는 모듈로 \n- 통상 self-signed TLS 인증서로 통신 가능하도록 설정해 줌 \n\n```bash\n# default 설치\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.161/cert-manager.yaml \n\n# 설치 확인\n$ k get po -n \nNAME                                        READY   STATUS  RESTART AGE\ncert-manager-54f448d676d-cgznw              1/1     Running 0       10d\ncert-manager-cainjector-7fc66c5787-gbpjk    1/1     Running 0       10d\ncert-manager-webhook-fb4d6fb7-7zn7j         1/1     Running 0       10d\n```\n\n### 4-3. `opentelemetry operator` 설치\n\n```bash\ncd ${WORKDIR}/opentelemetry-helm-charts/charts/opentelemetry-operator\n```\n#### 4-3-1. `values.yaml` 편집내용\n```yaml \n## 변경 내용 : image repostiroy \n....(중략)....\nmanager:\n  image:\n    repository: sharedprdacr.azureacr.io/open-telemetry/opentelemetry-operator/opentelemetry-operator\n    tag: \"\"\n  collectorImage:\n    repository: sharedprdacr.azureacr.io/open-telemetry/opentelemetry-operator/opentelemetry-collector\n    tag: 0.110.0\n....(생략)....\n\nkubeRBACProxy:\n  image:\n    repository: sharedprdacr.azureacr.io/brancz/kube-rbac-proxy\n\n```\n#### 4-3-2. `opentelemetry-collector` 설치\n\n- values.yaml 편집\n\n\n```yaml\n....(생략) - tempo 설정....\nmode: \"deployment\" # deployment로 변경\nconfig:\n  exporters:\n    otlp:\n      endpoint: \"http://tempo.ns-prometheus.svc.cluster.local:4317\"  ## tempo를 이용할 것으로 exporter로 tempo를 지정 , Tempo 설치는 다음절 참조\n      tls:\n        insecure: true\n# 변경 사항은 아니지만, 중요 설정 \n  processors:\n    batch: {} \n    memory_limiter:\n      check_interval: 5s\n      limit_percentage: 80\n      spike_limit_percentage: 25\n  receivers:\n    jaeger: # jaeger\n      protocols: \n        grpc: \n          endpoint: ${env:MY_POD_PI}:14250\n        thrift_http:\n          endpoint: ${env:MY_POD_IP}:14268\n        thrift_compact:\n          endpoint: ${env:MY_POD_IP}:6831\n    otlp: # otlp\n      protocols:\n        grpc:\n          endpoint: ${env:MY_POD_IP}:4317\n        http:\n          endpoint: ${env:MY_POD_IP}:4318\n    prometheus:\n      config:\n        scrape_configs:\n        - job_name: opentelemetry_collector\n          scrape_interval: 10s\n          static_configs:\n          - targets:\n            - ${env:MY_POD_IP}:8888\n    zipkin:\n      endpoint: ${env:MY_POD_IP}:9411\n  service:\n    telemetry:\n      metrics:\n        address: ${env:MY_POD_IP}:8888\n    extensions:\n      - health_check\n    pipelines:\n      logs:\n        exportors:\n          - debug\n        processors:\n          - memory_limiter\n          - batch\n        receivers:\n          - otlp\n      metrics:\n        exporters:\n          - debug\n        processors:\n          - memory_limiter\n          - batch\n        receivers:\n          - otlp\n          - prometheus\n      traces:\n        exportoers:\n          - debug\n        processors:\n          - memory_limiter\n          - batch\n        receivers:\n          - otlp\n          - jaeger\n          - zipkin\n```\n \n- `helm install` \n\n\n```bash\ncd ${WORKDIR}/opentelemetry-helm-charts/charts/opentelemetry-collector\nhelm upgrade --install otel-collector . -f values.yaml\n```\n\n### 4-4. `opentelemetry-instatruments` 설정\n- instruments 설정 방벙에는 여러 가지가 있으며, 앞서 살펴본 바와 같이 auto-instruments를 설정하여 resource 생성 시 tag를 활용하여 자동 주입되는 환경은 아니다. \n- 해당 설정을 위해서는 **certManager Webhook** 설정이 필요하며 이는 **cluster 의 8443 port open 필요**\n- 여기서는 instrument를 별도로 설치하지 않고, 보내는 서비스에서 lib 파일을 agent 형태로 추가하여 collector에 전송하는 방법을 택했다. \n- 참고 사항으로 auto-instruments 설정을 위한 manifests 파일은 아래와 같다. \n\n#### 4-4-1. [참고] otel-instrumentation \n```yaml\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: otel-instrumentation \nspec:\n  exporter:\n    endpoint: \" http://otel-collector-opentelemetry-collector.ns-prometheus:4317\"\n  propagators:\n    - tracecontext\n    - baggage\n  sampler:\n    type: parentbased_traceidratio\n    argument: \"0.25\"\n  python:\n    env:\n    - name: OTEL_EXPORTER_OTLP_ENDPOINT\n      value: http://otel-collector-opentelemetry-collector.ns-prometheus:4318\n  dotnet:\n    env:\n    - name: OTEL_EXPORTER_OTLP_ENDPOINT\n      value: http://otel-collector-opentelemetry-collector.ns-prometheus:4318\n  go:\n    env:\n    - name: OTEL_EXPORTER_OTLP_ENDPOINT\n      value: http://otel-collector-opentelemetry-collector.ns-prometheus:4318\n```\n\n> [참고] otel-instrumentation 설치를 위한 명령어 \n```bash\nkubectl config use-context <<cluster-context>>;\nhelm upgrade --install --set \"manager.collectorImage.repository=sharedprdacr.azureacr.io/opentelemetry-collector-k8s\" \\\n                       --set \"admissionWebhooks.certManager.enabled=false \\ \n                       --set admissionWebhooks.autoGenerateCert.enabled=true\n                       opentelemetry-operator . -f values.yaml -n ns-prometheus \n```\n\n\n> [참고] otel operator 재설치할 경우 삭제가 필요한 CRD 목록\n\n| 대상 | 삭제 명령 | \n|--|--|\n| instrumentations.opentelemetry.io | kubectl delete crd instrumentations.opentelemetry.io |\n| opampbridges.opentelemetry.io | kubectl delete crd opampbridges.opentelemetry.ioopampbridges.opentelemetry.io |\n| opentelemetrycollectors.opentelemetry.io | kubectl delete crd opentelemetrycollectors.opentelemetry.io | \n\n\n## 5. `Tempo` 설치\n- Tempo는 `trace` 를 위한 OSS library 중 하나로 grafana community version에서 확인 가능 \n\n### 5-1. `values.yaml` 편짐 \n```yaml\n....(생략)....\ntempo:\n  metricsGenerator:\n    enabled: true\n    repoteWriteUrl: \"http://prometheus-kube-promethus-prometheus.ns-prometheus:9090/api/v1/write\"\n  ........\n  retention: 24h\n  storage:\n    trace:\n      backend: azure\n      azure:\n        container_name: grafana-tempo\n        storage_account_name: <<STORAGE_ACCOUNT_NAME>>\n        storage_account_key: <<STORAGE_ACCOUNT_KEY>>\n      local:\n        path: /var/tempo/traces\n      wal:\n        path: /var/tempo/wal\n  receivers:\n    opencensus:\n    otlp:\n      protocols:\n        grpc:\n          endpoint: \"0.0.0.0:4317\"\n        http:\n          endpoint: \"0.0.0.0:4318\"\n```\n\n### 5-2. `Tempo` install\n```bash\ncd ${WORKDIR}/grafana-stack/charts/tempo\nkubectl config use-context <<cluster-context>>; helm upgrade --install tempo . -n ns-prometheus\n```\n\n## 6. Exporter 설치\n### 6-1. `postgresql-exporter` 설치\n#### 6-1-1. 설정 값 변경\n```yaml\nimage:\n  registry: quay.io\n  repository: prometheuscommunity/postgres-exporter\n\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 9187\n  name: http\n  labels: {}\n  annotaions: \n    prometheus.io/path: /metrics\n    prometheus.io/scrape: \"true\"\nserviceMonitor:\n  enabled: true\n  namespace: ns-prometheus\n  interval: 30s\n  telemetryPath: /metrics\n  labels: pg-dev\n  timeout: 10s\nprometheusRule:\n  enabled: false\n```\n\n#### 6-1-2. `postgresql-exporter` 설치\n```bash\ncd ${WORKDIR}/postgres_exporter/prometheus-postgre-exporter\nhelm upgrade --install prometheus-postgres-exporter . -f postgre-exporter.yaml -n ns-prometheus \n```\n\n#### 6-1-3. `prometheus exporter` 설정\n```yaml\n# prometheus config \n....\nprometheusSpec:\n  additionalScrapeConfigs:\n  ## for pg\n  - job_name: pg_exporter\n    metrics_path: /metrics\n    scrape_interval: 60s\n    scrape_timeout: 30s\n    static_configs:\n    - targets:\n      - prometheus-postgres-exporter:80\n  ## for redis \n  - job_name: redisexporter\n    static_configs:\n    - targets:\n      - redis-exporter-prometheus-reids-exporter:9121\n```\n### 6-2. `redis-exporter` 설치\n- oliver006/redis_exporter 활용 : [github주소](https://github.com/oliver006/redis_exporter)\n\n#### 6-2-1. `values.yaml` 편집 \n```yaml\n# 확인 \nservice:\n  type: ClusterIP\n  port: 9121\n  portName: redis-exporter\n```\n## 7. [참고 사이트]\n\n## 8. Epilog\n- k8s 서비스 노출을 L7 LB를 통하여 `domain`을 열어야 하는 구조인데, 해당 기능 승인을 정보보안팀에서 담당하여, 하나 `domain` 해제하기 까다롭고 절차도 오래 걸려, `manifest` 파일을 ~~한땀 한땀 수정하여~~ 구축하였으나 ~~삽질 인듯...~~\n- `multi-cluster` 적용과 multicluster를 통합한 `Thanos` 적용기도 추가할 예정이다. \n\n\n---"},{"id":"/2024/11/01/fifth-Redis-cluster","metadata":{"permalink":"/2024/11/01/fifth-Redis-cluster","source":"@site/blog/2024-11-01-fifth-Redis-cluster.md","title":"Redis A-S(Active-Standby/Master-Slave) cluster 구축기","description":"summary","date":"2024-11-01T16:16:01.000Z","tags":[{"inline":false,"label":"Redis Cluster","permalink":"/tags/Redis Cluster","description":"Redis Cluster tag description"},{"inline":false,"label":"kubernetes","permalink":"/tags/kubernetes","description":"kubernetes tag description"}],"readingTime":3.285,"hasTruncateMarker":true,"authors":[{"name":"Hanbyul Cho","title":"Engineer","url":"https://github.com/hansgun","page":{"permalink":"/authors/hansgun"},"socials":{"linkedin":"https://www.linkedin.com/in/hanbyulcho1/","github":"https://github.com/hansgun"},"imageURL":"https://github.com/hansgun.png","key":"hansgun"}],"frontMatter":{"layout":"single","title":"Redis A-S(Active-Standby/Master-Slave) cluster 구축기","date":"2024-11-01 16:16:01 -0600","categories":"work","tags":["Redis Cluster","kubernetes"],"author_profile":false,"toc":true,"toc_label":"On This Page","toc_icon":"cog","toc_position":"sticky","authors":["hansgun"]},"unlisted":false,"prevItem":{"title":"Setup Prometheus on private k8s cluster","permalink":"/2024/12/18/forth-prometheus"},"nextItem":{"title":"AKS terraform으로 시작해보기","permalink":"/2024/10/26/sixth-terrform-aks"}},"content":"> summary\n<!-- truncate -->\n\n# Prolog \n- 과제로 작성한 내용임 \n- OSS DB 혹은 cache 솔루션은 성능, 효율성을 강조하는 대신, 정합성을 포함한 안정성에 대해서는 상용 제품들에 비해 기본 기능에서 제공하지 않는 경우가 많다. \n- 고가용성 설정 Oracle RAC 등의 session failover 등의 detail 한 설정을 구현하기는 쉽지 않다. \n- OSS 레벨에서는 Primary에 문제가 생겼을 경우 Primary failover에 중점을 둔 replication 기반은 PSS 혹은 PSA 구조의 고가용성 구현이 지배적인 것 같다. \n- 이러한 구조는 primary 운영에 이슈가 생겼을 경우 vote 하는 절차에서 의미 있는 결과를 도출하기 위하여 `quorum` 기반의 알고리즘 이 영향을 미친 것 같다.   \n\n# 1. Task 개요\n----\n- key-value store 이자 cache로 많이 활용되는 Redis 는 메모리 기반의 휘발성 저장 솔루션이다. \n- persistency를 위하여 RDBMS와 같이 저장장치에 데이터 동기화를 주기적으로 수행할 수 있으나 이 경우 메모리 상의 데이터 변경이 제한된다. 성능 저하 이슈도 발생한다. \n  - 최근에는 ahead logging 방식으로 sequential 하게 log를 기록하여 오버헤드를 줄이고, 이를 streaming 형태로 Standby 서버에 전송하여 준실시간 동기화를 통한 고가용성을 구현할 수 있다. \n- sharding 과 같은 개념으로 key 값을 분산 처리 하여 부하 분산할 수 있도록 cluster 구성이 가능하다. \n- 이에 대한 구현을 아래와 같이 진행하였다. \n- 관리형 k8s cluster 에서 pod 형태로 redis cluster 구성과 모니터링을 구성하였다. \n\n1. GKE 구성\n2. Redis Cluster 구성 \n  - `StatefulSet` 활용 \n  - Primary-Secondary 구조 구성 \n3. Prometheus 설치 \n  - redis-exporter 설정 \n4. redis 접근 application 생성 및 HA 접근 설정, key 분산 저장 여부 확인 \n\n## 1-1. 구성도 \n![redis cluster 구성도](./cluster_archi.png)\n\n# 2. GKE 구성\n- GKE 구성은 `SDK` 설치 이후 `CLI` 를 통해 install 진행 \n\n```bash\n## env \nexport CLUSTER_NAME=\"gke-han-cluster\"\nexport ZONE=\"asia-east1\"\nexport NUM_NODES=2\nexport MACHINE_TYPE=\"e2-medium\"\n\ngcloud auth login q*****@gmail.com\n\ngcloud auth list\n\ngcloud projects list\n# PROJECT_ID: ninth-age-4*****-p0\n# NAME: My First Project\n# PROJECT_NUMBER: 39*******\n\nexport PROJECT_ID=\"your-project-id\"\ngcloud config set project $PROJECT_ID\n\ngcloud services enable container.googleapis.com\n\ngcloud container clusters create $CLUSTER_NAME \\ \n    --zone $ZONE \\\n    --num-nodes $NUM_NODES \\\n    --machine-type $MACHINE_TYPE \\\n    --enable-ip-alias \\\n    --network \"default\" --disk-size=50GB \\\n    --subnetwork \"default\"\n    --enable-workload-identity\n\ngcloud components install kubectl\n\ngcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE\n\n# add disk \ngcloud compute disks create --size=10GB --zone=asia-east1-a nfs-disk\n```\n\n# 3. k8s 워크로드 구성\n## 3-1. 파일 구조\n```bash\n$ tree v1 -L 1\n├── 00.disk.yaml\n├── 01.sts.yaml\n├── 02.svc.yaml\n├── 04.grafana_svc.yaml\n└── backup\n```\n# (작성중)\n# 7. [참고 사이트]\n\n# Epilog\n- \n\n---"},{"id":"/2024/10/26/sixth-terrform-aks","metadata":{"permalink":"/2024/10/26/sixth-terrform-aks","source":"@site/blog/2024-10-26-sixth-terrform-aks.md","title":"AKS terraform으로 시작해보기","description":"summary","date":"2024-10-26T16:16:01.000Z","tags":[{"inline":false,"label":"GKE","permalink":"/tags/GKE","description":"GKE tag description"},{"inline":false,"label":"kubernetes","permalink":"/tags/kubernetes","description":"kubernetes tag description"}],"readingTime":6.105,"hasTruncateMarker":true,"authors":[{"name":"Hanbyul Cho","title":"Engineer","url":"https://github.com/hansgun","page":{"permalink":"/authors/hansgun"},"socials":{"linkedin":"https://www.linkedin.com/in/hanbyulcho1/","github":"https://github.com/hansgun"},"imageURL":"https://github.com/hansgun.png","key":"hansgun"}],"frontMatter":{"layout":"single","title":"AKS terraform으로 시작해보기","date":"2024-10-26 16:16:01 -0600","categories":"work","tags":["GKE","kubernetes"],"author_profile":false,"toc":true,"toc_label":"On This Page","toc_icon":"cog","toc_position":"sticky","authors":["hansgun"]},"unlisted":false,"prevItem":{"title":"Redis A-S(Active-Standby/Master-Slave) cluster 구축기","permalink":"/2024/11/01/fifth-Redis-cluster"},"nextItem":{"title":"Authorizer Architecuture","permalink":"/2020/01/29/second-blog"}},"content":"> summary\n<!-- truncate -->\n\n# Prolog \n- Prometheus를 k8s cluster에 설치하는 실습을 진행하고자 설치 환경을 알아보던 중, Azure의 무료 credit이 있어 AKS를 설치해 보았다. \n- 평소 실습해보고자 했던 `Terraform` 을 이용하여 Provisioning 하는 history를 기록에 남기고자 작성한 blog 이다. \n- 실제 작업 후 2개월 이후에 작성하는 내용이라 기억이 가물가물 하다... \n\n# 1. 개 요\n----\n* Prometheus - [ kube-prometheus-stack ] \n* Loki - for log\n* Tempo, Opentelemetry Agent - for trace\n* Exporters - Redis, PostgreSQL\n\n## 1-1. 파일 구조 \n```bash\n$ tree\n.\n├── az_terra.sh\n└── terraform\n    ├── 0_variables.tf\n    ├── 1_providers.tf\n    ├── 2_ssh.tf\n    ├── 3_main.tf\n    ├── 7_output.tf\n    ├── main.tfplan\n    ├── terraform.tfstate\n    └── terraform.tfstate.backup\n```\n\n## 1-2. 전체 흐름 - `az_terra.sh` \n```bash\nexport USERNAME=********@hotmail.com\naz account list --query \"[?user.name=='********@hotmail.com'].{Name:name, ID:id, Default:isDefault}\" --output Table\n\naz group  list --query \"[?location=='koreacentral']\"\n\n[\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"b42795af-74d2-****-*****-****\",\n    \"id\": \"9e7c5e29-60c3-4a20-8afd-*********\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Microsoft Azure 스폰서쉽 2\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"b42795af-74d2-****-*****-****\",\n    \"user\": {\n      \"name\": \"********@hotmail.com\",\n      \"type\": \"user\"\n    }\n  }\n]\n\n## \nexport MSYS_NO_PATHCONV=1\n\naz account set --subscription \"9e7c5e29-60c3-******-****-********\"\n\naz ad sp create-for-rbac --name vmss_rbac --role Contributor --scopes /subscriptions/9e7c5e29-60c3-******-****-********\n\n{\n  \"appId\": \"a887e74b-****-4ae5-84d6-********\",\n  \"displayName\": \"vmss_rbac\",\n  \"password\": \"q5b8Q~D&&&&&&*****jec_k\",\n  \"tenant\": \"b42795af-74d2-****-*****-****\"\n}\n\n\nexport ARM_SUBSCRIPTION_ID=\"9e7c5e29-60c3-******-****-********\"\nexport ARM_TENANT_ID=\"b42795af-74d2-****-*****-****\"\nexport ARM_CLIENT_ID=\"a887e74b-****-4ae5-84d6-********\"\nexport ARM_CLIENT_SECRET=\"q5b8Q~D&&&&&&*****jec_k\"\n\n\naz ad sp create-for-rbac --name <service_principal_name> --role Contributor --scopes /subscriptions/<subscription_id>\n\n\nprovider \"azurerm\" {\n  features {}\n\n  subscription_id   = \"9e7c5e29-60c3-******-****-********\"\n  tenant_id         = \"b42795af-74d2-****-*****-****\"\n  client_id         = \"a887e74b-****-4ae5-84d6-********\"\n  client_secret     = \"q5b8Q~D&&&&&&*****jec_k\"\n}\n\nclient_certificate = <sensitive>\nclient_key = <sensitive>\ncluster_ca_certificate = <sensitive>\ncluster_password = <sensitive>\ncluster_username = <sensitive>\nhost = <sensitive>\nkey_data = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCuQxKT5JhDZD+************/r11DJCx/SlwguHFucV3R+wQSF7/3WwZaqANUS/ArN+zUoAjqmST/UfaJ5JBg4aehoHmzzzhVn/cqA3V31V2rJog5siIYYFFan19VgxsqS3/QH9kxU0/QjQ3PqLOe0hPrbENpnJ8t1BgszokaTPToaJrCMIoCzgY7mBWVnseqgG6qU8vowMM**************tmFKD/tCylxBTecfAsbJBiJt5lcx8eF5dIS**************I7IhAyeHKicexZbt82Lxu6tXwJERYBQ5lkX4Kp************iMuJQ9QEChPCKveKlmU+Tlg3n1P0fNbCo+oWCdRuP6hm4HrU/+Z**********rCCU= generated-by-azure\"\nkube_config = <sensitive>\nkubernetes_cluster_name = \"cluster-diverse-bear\"\nresource_group_name = \"rg-premium-koala\"\n```\n\n## 1-3. Terraform 설정 값 \n- `terraform-docs`로 생성함 \n\n```bash\n$ brew install terraform-docs\n...\n$ terraform-docs markdown table \\\n    --output-file README.md \\\n    --output-mode inject \\\n    .\n\nREADME.md updated successfully\n```\n\n### 1-3-1. README.md file 내용 - `terraform-docs` 생성파일\n---\n> <!-- BEGIN_TF_DOCS -->\n> ### Requirements\n> \n> | Name | Version |\n> |------|---------|\n> | <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | >=1.0 |\n> | <a name=\"requirement_azapi\"></a> [azapi](#requirement\\_azapi) | ~>1.5 |\n> | <a name=\"requirement_azurerm\"></a> [azurerm](#requirement\\_azurerm) | ~>3.0 |\n> | <a name=\"requirement_random\"></a> [random](#requirement\\_random) | ~>3.0 |\n> | <a name=\"requirement_time\"></a> [time](#requirement\\_time) | 0.9.1 |\n> \n> ### Providers\n> \n> | Name | Version |\n> |------|---------|\n> | <a name=\"provider_azapi\"></a> [azapi](#provider\\_azapi) | 1.12.1 |\n> | <a name=\"provider_azurerm\"></a> [azurerm](#provider\\_azurerm) | 3.93.0 |\n> | <a name=\"provider_random\"></a> [random](#provider\\_random) | 3.6.0 |\n> \n> ### Modules\n> \n> No modules.\n> \n> ### Resources\n> \n> | Name | Type |\n> |------|------|\n> | [azapi_resource.ssh_public_key](https://registry.terraform.io/providers/azure/azapi/latest/docs/resources/> resource) | resource |\n> | [azapi_resource_action.ssh_public_key_gen](https://registry.terraform.io/providers/azure/azapi/latest/docs/> resources/resource_action) | resource |\n> | [azurerm_kubernetes_cluster.k8s](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/> resources/kubernetes_cluster) | resource |\n> | [azurerm_resource_group.rg](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/> resources/resource_group) | resource |\n> | [random_pet.azurerm_kubernetes_cluster_dns_prefix](https://registry.terraform.io/providers/hashicorp/> random/latest/docs/resources/pet) | resource |\n> | [random_pet.azurerm_kubernetes_cluster_name](https://registry.terraform.io/providers/hashicorp/random/> latest/docs/resources/pet) | resource |\n> | [random_pet.rg_name](https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/pet) | > resource |\n> | [random_pet.ssh_key_name](https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/> pet) | resource |\n> \n> ### Inputs\n> \n> | Name | Description | Type | Default | Required |\n> |------|-------------|------|---------|:--------:|\n> | <a name=\"input_msi_id\"></a> [msi\\_id](#input\\_msi\\_id) | The Managed Service Identity ID. Set this value > if you're running this example using Managed Identity as the authentication method. | `string` | `null` | > no |\n> | <a name=\"input_node_count\"></a> [node\\_count](#input\\_node\\_count) | The initial quantity of nodes for the > node pool. | `number` | `2` | no |\n> | <a name=\"input_resource_group_location\"></a> [resource\\_group\\_location]> (#input\\_resource\\_group\\_location) | Location of the resource group. | `string` | `\"koreacentral\"` | no |\n> | <a name=\"input_resource_group_name_prefix\"></a> [resource\\_group\\_name\\_prefix]> (#input\\_resource\\_group\\_name\\_prefix) | Prefix of the resource group name that's combined with a random ID > so name is unique in your Azure subscription. | `string` | `\"rg\"` | no |\n> | <a name=\"input_username\"></a> [username](#input\\_username) | The admin username for the new cluster. | > `string` | `\"azureadmin\"` | no |\n> \n> ### Outputs\n> \n> | Name | Description |\n> |------|-------------|\n> | <a name=\"output_client_certificate\"></a> [client\\_certificate](#output\\_client\\_certificate) | n/a |\n> | <a name=\"output_client_key\"></a> [client\\_key](#output\\_client\\_key) | n/a |\n> | <a name=\"output_cluster_ca_certificate\"></a> [cluster\\_ca\\_certificate]> (#output\\_cluster\\_ca\\_certificate) | n/a |\n> | <a name=\"output_cluster_password\"></a> [cluster\\_password](#output\\_cluster\\_password) | n/a |\n> | <a name=\"output_cluster_username\"></a> [cluster\\_username](#output\\_cluster\\_username) | n/a |\n> | <a name=\"output_host\"></a> [host](#output\\_host) | n/a |\n> | <a name=\"output_key_data\"></a> [key\\_data](#output\\_key\\_data) | n/a |\n> | <a name=\"output_kube_config\"></a> [kube\\_config](#output\\_kube\\_config) | n/a |\n> | <a name=\"output_kubernetes_cluster_name\"></a> [kubernetes\\_cluster\\_name]> (#output\\_kubernetes\\_cluster\\_name) | n/a |\n> | <a name=\"output_resource_group_name\"></a> [resource\\_group\\_name](#output\\_resource\\_group\\_name) | n/a |\n>\n> <!-- END_TF_DOCS -->\n---\n\n### 1-3-2. `0-variables.tf` 파일 내용 중 변경 사항\n```hcl\nvariable \"resource_group_location\" {\n  type        = string\n  default     = \"koreacentral\"  ###### region 설정 ######\n  description = \"Location of the resource group.\"\n}\n\n\nvariable \"node_count\" {\n  type        = number\n  description = \"The initial quantity of nodes for the node pool.\"\n  default     = 2  ###### cluster node 갯수 설정 : minimal 설정 ######\n}\n\n\nvariable \"username\" {\n  type        = string\n  description = \"The admin username for the new cluster.\"\n  default     = \"azureadmin\" ###### username for cluster (default value 그대로 이용) ######\n}\n```\n\n### 1-3-3. `1_providers.tf` \n```hcl\nterraform {\n  required_version = \">=1.0\"\n\n  required_providers {\n    azapi = {\n      source  = \"azure/azapi\"\n      version = \"~>1.5\"\n    }\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~>3.0\"\n    }\n    random = {ㄹ\n      source  = \"hashicorp/random\"\n      version = \"~>3.0\"\n    }\n    time = {\n      source  = \"hashicorp/time\"\n      version = \"0.9.1\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n\n  subscription_id   = \"9e7c5e29-60c3-******-****-********\"  ###### 설정에 맞게 변경함 ######\n  tenant_id         = \"b42795af-74d2-****-*****-****\"       ###### 설정에 맞게 변경함 ######\n  client_id         = \"a887e74b-****-4ae5-84d6-********\"    ###### 설정에 맞게 변경함 ######\n  client_secret     = \"q5b8Q~D&&&&&&*****jec_k\"             ###### 설정에 맞게 변경함 ######\n}\n\n```\n\n### 1-3-4. `2_ssh.tf` : 변경사항 없음\n\n### 1-3-5. `3_main.tf` \n\n```hcl\n....(생략)....\n\n  default_node_pool {\n    name       = \"agentpool\"          ###### pool 이름 설정 ######\n    vm_size    = \"Standard_D2_v2\"     ###### vm size 설정 ######\n    node_count = var.node_count\n  }\n....(생략)....\n```\n\n### 1-3-6. `7_output.tf` : 변경 없음\n\n## 2. Terraform 실생 \n\n```mermaid\ngraph LR;\n  A[terraform init] -.-> B[terraform plan] -.-> C[terraform apply] -.-> D[terraform destroy];\n  A -.-> E[terraform validate] -.-> B;\n  C -.-> F[terraform output] -.-> G[terraform show];\n  G -.-> D;\n```\n\n```bash\n$ terraform init \n$ terraform plan \n$ terraform apply \n```\n\n## 7. [참고 사이트]\n- ~~기억이 안남~~\n\n## 8. Epilog\n- idempotent IaC 을 통해서 infra provisioning 관리를 해보고자 했는데, 실제 업무 환경에서는 보안적 문제와 반복 업무의 부재 등으로 도입 되지 않았다. side-project로 밖에 경험할 수 없을 것 같다. \n- 예전 Hadoop 설치의 복잡함을 해결하고자 ansible로 IaC 구성한 적이 있는데, 자료를 남기지 않았었고, 이전 회사의 경험이라 자료가 없어 기록으로 남기지 못해 아쉽다.  \n\n\n---"},{"id":"/2020/01/29/second-blog","metadata":{"permalink":"/2020/01/29/second-blog","source":"@site/blog/2020-01-29-second-blog.md","title":"Authorizer Architecuture","description":"summary","date":"2020-01-28T06:38:01.000Z","tags":[{"inline":false,"label":"docker-compose","permalink":"/tags/docker-compose","description":"docker-compose tag description"},{"inline":false,"label":"authorizer","permalink":"/tags/authorizer","description":"authorizer tag description"}],"readingTime":7.495,"hasTruncateMarker":true,"authors":[{"name":"Hanbyul Cho","title":"Engineer","url":"https://github.com/hansgun","page":{"permalink":"/authors/hansgun"},"socials":{"linkedin":"https://www.linkedin.com/in/hanbyulcho1/","github":"https://github.com/hansgun"},"imageURL":"https://github.com/hansgun.png","key":"hansgun"}],"frontMatter":{"layout":"post","title":"Authorizer Architecuture","date":"2020-01-28 06:38:01 -0600","author_profile":false,"categories":"work","tags":["docker-compose","authorizer"],"permlink":"/categories/","toc":true,"toc_label":"On This Page","toc_icon":"cog","authors":["hansgun"]},"unlisted":false,"prevItem":{"title":"AKS terraform으로 시작해보기","permalink":"/2024/10/26/sixth-terrform-aks"},"nextItem":{"title":"Setup CI/CD [Jenkins+Nexus for Spring, Docker, R]","permalink":"/2019/11/17/first-blog"}},"content":"> summary\n<!-- truncate -->\n\n> _project 구현 기반. single server에 single proxy, 2-java Oauth container를 연결한 형태임_\n\n# 1. 기본 개념\n\n- 외부 통신은 단일 interface를 통하여 수행. (nginX reverse proxy 이용, port 8078[현재버전])\n- 실제 authorizer component는 이중화 구성하며, 외부에 노출되는 port는 없음\n- 위 2가지 component는 health check fail 시 자동으로 재기동. (현재는 proxy만 설정된 상태 ==> restart:always)\n- cache layer는 redis 를 활용한다.\n\n# 2. 구축 방안\n\n- 개별 component는 docker container를 이용.\n- 하나의 서비스화를 위하여 docker-compose로 구성\n- 현 버전은 하나의 물리 서버에서 docker container를 여러 개 띄우는 구성이며, 물리 서버의 cluster화 할 경우 추가 solution 필요\n\n## 2-1. implementation\n\n- $HOME directory 관련 파일 구조\n\n```\n├── Dockerfile ## authorizer 생성 Dockerfile\n├── Dockerfile_proxy ## proxy(nginx) 생성 Dockerfile\n├── docker-compose.yml ## docker-compose file\n├── make_docker_compose.sh ## 전체 프로세스를 수행하기 위한 shell script\n├── nginx.conf ## Dockerfile_proxy 에서 사용할 nginx config 파일\n├── .env ## docker-compose 환경 변수\n└── target ## authorizer jar 파일이 생성되는 위치\n```\n\n- 설치 방법\n\n```\nsh make_docker_compose.sh <설치된 물리서버 IP address> ## IP address는 .env에서 redis에서 활용\n```\n\n## 2-2. 각 파일의 내용\n\n### 2-2-1. shell script\n\n`make_docker_compose.sh`\n\n```bash\n#!/bin/bash\n\n## check parameter length\nif [ \"$#\" -gt 3 ]; then\n        echo \"$# is Illegal number of parameters.\"\n        echo \"Usage: $0 [authoizer1_host_ip] [authorizer2_host_ip]\"\n        exit 1\nfi\nargs=(\"$@\")\n\n## print parameter list\nfor (( c=0; c<$#; c++   ))\n  do\n    echo \"$c th parameter = ${args[$c]}\";\n  done\n\n## set-up .env file\necho \"APP_HOST_NAME=auth_app\" > .env\nif [ \"$#\" -eq 2 ]; then\n        echo \"APP_HOST_IP1=${args[0]}\" >> .env\n        echo \"APP_HOST_IP2=${args[1]}\" >> .env\nelif [ \"$#\" -eq 1 ]; then\n        echo \"APP_HOST_IP1=${args[0]}\" >> .env\n        echo \"APP_HOST_IP2=${args[0]}\" >> .env\nelse\n        echo \"APP_HOST_IP1=127.0.0.1\" >> .env\n        echo \"APP_HOST_IP2=127.0.0.1\" >> .env\nfi\n\n## build jar file for authorizer\nmvn -e -DskipTests=true clean install\n\n## docker-compose up\ndocker-compose up --build -d\n\n```\n\n### 2-2-2. docker-compose 관련 파일\n\n- docker-compose.yml : service 전체 내용 및 dependency 실행 순서 등 container 제어\n- .env : docker-compose.yml 에서 사용할 환경 변수 정의\n- Dockerfile : authorizer(springboot OAuth2 server) container image\n- Dockerfile_proxy : nginx 설치 이미지로 다음 절(2-2-3에서 설명)\n\n`docker-compose.yml`\n\n- 순서대로 redis, authorizer1, authorizer2, proxy 4개의 이미지를 생성\n- proxy service 구성 중 links 부분은 명명된 service를 ip가 아닌 name으로 접근 가능하도록 설정함\n\n\n```yaml\n# VERSION 1.0.0\n# AUTHOR: project\n# DESCRIPTION: project Authorizer\n# BUILD: docker-compose -p <name> -d up -build\n# SOURCE: https://github.com/\n\nversion: '2.1'\n\nservices:\n#  redis:\n#    image: redis:3.2.12\n#    ports:\n#      - \"6379:6379\"\n\n  authorizer1:\n    #hostname: authorizer1\n    #container_name: authorizer1\n    build:\n      context: .\n    environment:\n      - REDIS_HOST=redis\n      - TZ=Asia/Seoul\n    extra_hosts:\n      - ${APP_HOST_NAME}:${APP_HOST_IP1} ## .env 에 정의된 변수 사용\n    #ports: ## port expose test.. authorizer container 외부로 open 되는 port 없음\n    #  - \"8085:8081\"\n#    depends_on:\n#      - redis\n\n  authorizer2:\n    build:\n      context: .\n    environment:\n      - REDIS_HOST=redis\n      - TZ=Asia/Seoul\n    extra_hosts:\n      - ${APP_HOST_NAME}:${APP_HOST_IP2}\n    #ports:\n    #  - \"8086:8081\"\n#    depends_on:\n#      - redis\n\n  proxy:\n    restart: always\n    build:\n      context: .\n      dockerfile: Dockerfile_proxy\n#    ports:\n#      - \"8084:80\" ## nginX 80 port를 외부 8084 port로 expose\n    depends_on:\n      - authorizer1\n      - authorizer2\n    links: ## docker 내부 네트워크에서 접근하기 위하여 hostname 정의\n      - authorizer1:authorizer1\n      - authorizer2:authorizer2\n    #extra_hosts:\n    #  - \"authorizer1:${APP_HOST_IP1}\"\n    #  - \"authorizer2:${APP_HOST_IP2}\"\n    healthcheck:\n      test: \"curl --fail http://localhost:8084/ || exit 1\"\n      interval: 1m\n      timeout: 1s\n      retries: 5\n```\n\n### 2-2-3. NginX 관련\n\n- Dockerfile_proxy : nginx image를 생성하는 dockerfile\n- nginx.conf : nginx 에서 사용할 config 를 미리 정의한 내용\n\n`Dockerfile_proxy`\n\n```\n# VERSION v.1.0.0\n# AUTHOR: hansgun\n# DESCRIPTION: project authorizer proxy\n# BUILD: docker build --rm -t project/proxy .\n# SOURCE: https://github.com/\n\nFROM nginx:latest\n\nCOPY ./nginx.conf /etc/nginx/ ## current directory 에서 conf 파일 복사\n```\n\n`nginx.conf`\n\n- \\*\\* 특이점은 nginx 에서 authorizer1,2 docker 접근 시 IP가 아닌 hostname으로 접근하여, container 내부의 port를 직접 접근 함\n\n```\n# /etc/nginx/nginx.conf\n\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include mime.types;\n\n    upstream myapp1 {\n        least_conn;\n        server authorizer1:8081 max_fails=1 fail_timeout=2s; # authorizer1,2 에 대한 load balancing 설정\n        server authorizer2:8081 max_fails=1 fail_timeout=2s; # docker네트워크에서 hostname으로 접근. IP, port 불필요\n    }\n\n    server {\n        listen 8078;\n        access_log /var/log/nginx/tomcat_access.log;\n        location /spring-security-oauth-server { ## authorizer application.yml 의 spring.servlet.context-path\n            proxy_pass http://myapp1;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_set_header X-NginX-Proxy true;\n                proxy_set_header Host $host:$server_port;\n                proxy_redirect off;\n        }\n    }\n}\n```\n\n\n## 3. 실행결과\n\n`sh make_docker_compose.sh 192.168.1.168 <-- IP는 환경에 맞게 변경 필요`\n\n\n```\n0 th parameter = 192.168.1.168\n[INFO] Error stacktraces are turned on.\n[INFO] Scanning for projects...\n[INFO]\n[INFO] -------------< kr.co.project.feedernet:projectAuthorizer >--------------\n[INFO] Building projectAuthorizer 1.0.0-SNAPSHOT\n[INFO] --------------------------------[ jar ]---------------------------------\n... 중간 생략 ...\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  5.199 s\n[INFO] Finished at: 2020-01-03T09:43:27+09:00\n[INFO] ------------------------------------------------------------------------\nBuilding authorizer1\nStep 1/12 : FROM java:8-jre\n ---> e44d62cf8862\nStep 2/12 : ENV APP_HOME /usr/lib/authorizer\n ---> Using cache\n ---> 4da19107a7c2\nStep 3/12 : RUN echo \"Asia/Seoul\" > /etc/timezone\n ---> Using cache\n ---> d6d7ac7e2220\nStep 4/12 : RUN dpkg-reconfigure -f noninteractive tzdata\n ---> Using cache\n ---> 3b9bc1f0b6aa\nStep 5/12 : RUN useradd -ms /bin/bash -d ${APP_HOME} authorizer\n ---> Using cache\n ---> 2bad24c3166b\nStep 6/12 : ARG JAR_FILE=\"target/*.jar\"\n ---> Using cache\n ---> 5f886986b5a2\nStep 7/12 : COPY ${JAR_FILE} ${APP_HOME}/authorizer.jar\n ---> 76556a94980a\nStep 8/12 : RUN chown authorizer: -R ${APP_HOME}\n ---> Running in 6f4cc39bc255\nRemoving intermediate container 6f4cc39bc255\n ---> a16310494008\nStep 9/12 : EXPOSE 8081\n ---> Running in 74d861e17a9f\nRemoving intermediate container 74d861e17a9f\n ---> f0a15efc2613\nStep 10/12 : USER authorizer\n ---> Running in f9b2f072847a\nRemoving intermediate container f9b2f072847a\n ---> 13821a47ae94\nStep 11/12 : WORKDIR ${APP_HOME}\n ---> Running in 6a1cc49da232\nRemoving intermediate container 6a1cc49da232\n ---> 8f3abf8226fe\nStep 12/12 : ENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-Xms256m\",\"-Xmx512m\",\"-server\",\"-XX:+UseNUMA\",\"-XX:+UseParallelGC\",\"-XX:+AggressiveOpts\",\"-XX:+UseFastAccessorMethods\",\"-jar\",\"authorizer.jar\"]\n ---> Running in ce9aa6573758\nRemoving intermediate container ce9aa6573758\n ---> 759031b1bfd8\nSuccessfully built 759031b1bfd8\nSuccessfully tagged projectauthorizer_authorizer1:latest\nBuilding authorizer2\nStep 1/12 : FROM java:8-jre\n ---> e44d62cf8862\nStep 2/12 : ENV APP_HOME /usr/lib/authorizer\n ---> Using cache\n ---> 4da19107a7c2\nStep 3/12 : RUN echo \"Asia/Seoul\" > /etc/timezone\n ---> Using cache\n ---> d6d7ac7e2220\nStep 4/12 : RUN dpkg-reconfigure -f noninteractive tzdata\n ---> Using cache\n ---> 3b9bc1f0b6aa\nStep 5/12 : RUN useradd -ms /bin/bash -d ${APP_HOME} authorizer\n ---> Using cache\n ---> 2bad24c3166b\nStep 6/12 : ARG JAR_FILE=\"target/*.jar\"\n ---> Using cache\n ---> 5f886986b5a2\nStep 7/12 : COPY ${JAR_FILE} ${APP_HOME}/authorizer.jar\n ---> Using cache\n ---> 76556a94980a\nStep 8/12 : RUN chown authorizer: -R ${APP_HOME}\n ---> Using cache\n ---> a16310494008\nStep 9/12 : EXPOSE 8081\n ---> Using cache\n ---> f0a15efc2613\nStep 10/12 : USER authorizer\n ---> Using cache\n ---> 13821a47ae94\nStep 11/12 : WORKDIR ${APP_HOME}\n ---> Using cache\n ---> 8f3abf8226fe\nStep 12/12 : ENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-Xms256m\",\"-Xmx512m\",\"-server\",\"-XX:+UseNUMA\",\"-XX:+UseParallelGC\",\"-XX:+AggressiveOpts\",\"-XX:+UseFastAccessorMethods\",\"-jar\",\"authorizer.jar\"]\n ---> Using cache\n ---> 759031b1bfd8\nSuccessfully built 759031b1bfd8\nSuccessfully tagged projectauthorizer_authorizer2:latest\nBuilding proxy\nStep 1/2 : FROM nginx:latest\n ---> f949e7d76d63\nStep 2/2 : COPY ./nginx.conf /etc/nginx/\n ---> Using cache\n ---> de0942485c9f\nSuccessfully built de0942485c9f\nSuccessfully tagged projectauthorizer_proxy:latest\nRecreating projectauthorizer_redis_1 ... done\nRecreating projectauthorizer_authorizer2_1 ... done\nRecreating projectauthorizer_authorizer1_1 ... done\nRecreating projectauthorizer_proxy_1       ... done\n```\n\n`docker-compose ps`\n\n```\n             Name                            Command                       State                   Ports\n-----------------------------------------------------------------------------------------------------------------\nprojectauthorizer_authorizer1_1   java -Djava.security.egd=f ...   Up                      8081/tcp\nprojectauthorizer_authorizer2_1   java -Djava.security.egd=f ...   Up                      8081/tcp\nprojectauthorizer_proxy_1         nginx -g daemon off;             Up (health: starting)   0.0.0.0:8084->8078/tcp\nprojectauthorizer_redis_1         docker-entrypoint.sh redis ...   Up                      0.0.0.0:6379->6379/tcp\n\n```\n\n`docker network inspect projectauthorizer_default`\n\n```js\n[\n    {\n        \"Name\": \"projectauthorizer_default\",\n        \"Id\": \"ab7956cbd8e787be575c050acd842854ed18906bf89c9566b929b818a90fe006\",\n        \"Created\": \"2020-01-02T08:28:09.400768103Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.29.0.0/16\",\n                    \"Gateway\": \"172.29.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": true,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"13dba477f3abd8b6d346b956a2787561a7affc29eca268234d463231db0e4d16\": {\n                \"Name\": \"projectauthorizer_proxy_1\",\n                \"EndpointID\": \"16cd689ed587cb92b19813337a3944cccf7d5685c080ddbc6c25f1a7fbe93b91\",\n                \"MacAddress\": \"02:42:ac:1d:00:05\",\n                \"IPv4Address\": \"172.29.0.5/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"591a74c933dd05b9dccb29b2b56dd58dc5d42e1f1e6a0c3fdb54e7b384f103b2\": {\n                \"Name\": \"projectauthorizer_authorizer1_1\",\n                \"EndpointID\": \"13633a2221edf6874fecb11b1a7dea0c0ddbdf5ac80cacb27c59ffd06225784c\",\n                \"MacAddress\": \"02:42:ac:1d:00:03\",\n                \"IPv4Address\": \"172.29.0.3/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"6702531cb6602b36086c570dd12f830ba76a7279426d7a523056b77b902d17b4\": {\n                \"Name\": \"projectauthorizer_redis_1\",\n                \"EndpointID\": \"2fcaa3885607fdee1f46ea95f97ec534f71d38945fc347663a78c599bfdd7146\",\n                \"MacAddress\": \"02:42:ac:1d:00:02\",\n                \"IPv4Address\": \"172.29.0.2/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"a42280ac856a5778395a34dab634e3e511ff1fd45b98cd68e0946640193d3bd6\": {\n                \"Name\": \"projectauthorizer_authorizer2_1\",\n                \"EndpointID\": \"a9c34a66279c465706351d252745055037d5f0c819670e9a7fd10de3b0709179\",\n                \"MacAddress\": \"02:42:ac:1d:00:04\",\n                \"IPv4Address\": \"172.29.0.4/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {},\n        \"Labels\": {\n            \"com.docker.compose.network\": \"default\",\n            \"com.docker.compose.project\": \"projectauthorizer\",\n            \"com.docker.compose.version\": \"1.24.1\"\n        }\n    }\n]\n```\n\n## 4. 접근 테스트\n\n`http://<서버 IP>:8078/spring-security-oauth-server/`"},{"id":"/2019/11/17/first-blog","metadata":{"permalink":"/2019/11/17/first-blog","source":"@site/blog/2019-11-17-first-blog.md","title":"Setup CI/CD [Jenkins+Nexus for Spring, Docker, R]","description":"summary","date":"2019-11-17T16:16:01.000Z","tags":[{"inline":false,"label":"jenkins","permalink":"/tags/jenkins","description":"jenkins tag description"},{"inline":false,"label":"nexus OSS","permalink":"/tags/nexus OSS","description":"nexus OSS tag description"},{"inline":false,"label":"CI/CD","permalink":"/tags/CI/CD","description":"CI/CD tag description"}],"readingTime":5.965,"hasTruncateMarker":true,"authors":[{"name":"Hanbyul Cho","title":"Engineer","url":"https://github.com/hansgun","page":{"permalink":"/authors/hansgun"},"socials":{"linkedin":"https://www.linkedin.com/in/hanbyulcho1/","github":"https://github.com/hansgun"},"imageURL":"https://github.com/hansgun.png","key":"hansgun"}],"frontMatter":{"layout":"post","title":"Setup CI/CD [Jenkins+Nexus for Spring, Docker, R]","date":"2019-11-17T16:16:01.000Z","categories":"work","tags":["jenkins","nexus OSS","CI/CD"],"author_profile":false,"toc":true,"toc_label":"On This Page","toc_icon":"cog","toc_position":"sticky","authors":["hansgun"]},"unlisted":false,"prevItem":{"title":"Authorizer Architecuture","permalink":"/2020/01/29/second-blog"},"nextItem":{"title":"Set up project dev environments","permalink":"/2020/01/30/third-blog"}},"content":"> summary\n<!-- truncate -->\n\n## 0. 구성요소\n\n- jenkins\n- nexus OSS\n- ansible\n- docker\n\n## 1. Jenkins 설치\n\n### 1-1. 다운로드 및 실행\n\n```bash\nwget http://mirrors.jenkins.io/war-stable/latest/jenkins.war\n#java -jar jenkins.war --httpPort=8080 --prefix=/jenkins\n```\n\n### 1-2. 접속\n\n```\nhttp://<hostname>:8080/jenkins\n```\n\n### 1-3. slave 설정\n\n> `UI 에서 Jenkins 관리 > 노드관리 로 옮긴후 신규노드 를 열어봅니다:`  \n> `1. 노드명을 입력: 예를들어 slave-01`  \n> `2. Permanent Agent 를 선택`  \n> `설정 Page 에서:`  \n> `1. Remote root directory 에 입력, 예를들어, /opt/jenkins.`  \n> `2. Launch method 는 Launch Slave Agents via SSH 를 선택, host 명 입력후 credential 추가`  \n> `2.1 Credential 추가시 Slave node user, jenkins 와 그 password 를 등록함`  \n> `2.2 혹은 SSH Username with private key 선택, From the jenkins master ~/.ssh  선택`  \n> `2.2.1 master 에서`\n\n```bash\nsudo su - jenkins;\n\n# generate key.\nssh-keygen -t rsa;\n\n# copy master public key to slave.\nssh-copy-id -i ~/.ssh/id_rsa.pub <hostname>; # 혹은 cat ~/.ssh/id_pub.rsa > authorized_keys\n\n# add config to ~/.ssh\nvi ~/.ssh/config;\nStrictHostKeyChecking no\n\n# chmod.\nchmod 600 ~/.ssh/config\n\n# check connection to slave.\nssh emb-a01;\n```\n\n## 2. nexus 설치\n\n```bash\n# download.\n# https://www.sonatype.com/download-oss-sonatype\n\n# change data directory.\n# https://help.sonatype.com/repomanager3/installation/configuring-the-runtime-environment#ConfiguringtheRuntimeEnvironment-ConfiguringtheDataDirectory\ncd <nexus-home>/bin;\n\nvi nexus.vmoptions;\n-Xms2703m\n-Xmx2703m\n-XX:MaxDirectMemorySize=2703m\n-XX:+UnlockDiagnosticVMOptions\n-XX:+UnsyncloadClass\n-XX:+LogVMOutput\n-XX:LogFile=../sonatype-work/nexus3/log/jvm.log ## 변경\n-XX:-OmitStackTraceInFastThrow\n-Djava.net.preferIPv4Stack=true\n-Dkaraf.home=.\n-Dkaraf.base=.\n-Dkaraf.etc=etc/karaf\n-Djava.util.logging.config.file=etc/karaf/java.util.logging.properties\n-Dkaraf.data=../sonatype-work/nexus3 ## 변경\n-Djava.io.tmpdir=../sonatype-work/nexus3/tmp ## 변경\n-Dkaraf.startLocalConsole=false\n```\n\n## 3. nexus docker registry 설정\n\n> Reference: [config reference](https://www.ivankrizsan.se/2016/06/09/create-a-private-docker-registry/)\n\n## 4. nexus 설정용 pom file\n\n### 4-1. pom.xml\n\n```xml\n    <repositories>\n        <repository>\n            <id>central</id>\n            <url>http://localhost:8081/repository/maven-public/</url>\n            <snapshots>\n                <enabled>true</enabled>\n                <updatePolicy>always</updatePolicy>\n            </snapshots>\n        </repository>\n    </repositories>\n    <pluginRepositories>\n        <pluginRepository>\n            <id>central</id>\n            <url>http://localhost:8081/repository/maven-public/</url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </pluginRepository>\n    </pluginRepositories>\n    <distributionManagement>\n        <snapshotRepository>\n            <id>snap</id>\n            <url>http://localhost:8081/repository/maven-snapshots/</url>\n        </snapshotRepository>\n        <repository>\n            <id>rel</id>\n            <url>http://localhost:8081/repository/maven-releases/</url>\n        </repository>\n    </distributionManagement>\n```\n\n### 4-2. mvn setting..\n\n```shell\n# mvn -version 으로 maven_home 확인 후\ncd $MAVEN_HOME.conf\nvi settings.xml\n```\n\n```xml\n<servers>\n    <server>\n        <id>central</id>\n        <username>$(nexus_username)</username>\n        <password>$(nexus_passwd)</password>\n    </server>\n    <server>\n        <id>snap</id>\n        <username>$(nexus_username)</username>\n        <password>$(nexus_passwd)</password>\n    </server>\n\t<server>\n        <id>rel</id>\n        <username>$(nexus_username)</username>\n        <password>$(nexus_passwd)</password>\n    </server>\n</servers>\n```\n\n### 4-3. Deploy test\n\n```console\nmvn -e -DskipTests=true clean install deploy;\n```\n\n## 5. 배포 대상 서버 설정\n\n### 5-1. docker private registry 등록\n\n```ksh\n# /etc/docker/daemon.json 파일 편집하여 secure 예외 처리 등록\n{\n    \"insecure-registries\": [\"<ip>:<port>\",\"<domain>:<port>\"]\n}\n\n# 저장후 docker restart\n# wheel group user\nsudo systemctl stop docker\nsudo systemctl start docker\n\n# <userID> 계정(group id : docker) 에서 정보 확인\n# docker info의 마지막 섹션 확인\ndocker info\n------------------------------------\n....(생략)....\n Insecure Registries:\n  <ip or domain>:<port>\n   127.0.0.0/8\n Live Restore Enabled: false\n------------------------------------\n```\n\n### 5-2. private registry login\n\n```sh\n# private docker registry login\n# docker login -u<userid> -p<password> <ip or domain>:<port>\ndocker login -uadmin -p <ip or domain>:<port>\npassword:\n```\n\n### 5-3. image pull & 확인\n\n```sh\n# docker image pull\ndocker pull <ip or domain>:<port>/app:1.0.0-SNAPSHOT\n\n# image 확인\ndocker images | grep -i jenkins\n------------------------------------\n<ip or domain>:<port>/app   1.0.0-SNAPSHOT      ec62cef80ecf        2 hours ago         235MB\n------------------------------------\n```\n\n## 6. Nexus R-plugin 설정\n\n### 6-1. plugin 다운로드\n\n#### requirements\n\n- Apache Maven 3.3.3+\n- OpenJDK 8\n- Network access to https://repository.sonatype.org/content/groups/sonatype-public-grid\n- nexus version과 호환되는 plugin 을 다운 받는다\n\n#### nexus,R plugin version\n\n<table>\n<thead>\n<tr>\n<th>Plugin Version</th>\n<th>Nexus Repository Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>v1.0.0</td>\n<td>&lt;3.8.0-02</td>\n</tr>\n<tr>\n<td>v1.0.1</td>\n<td>&gt;=3.8.0-02</td>\n</tr>\n<tr>\n<td>v1.0.2</td>\n<td>&gt;=3.14.0-04</td>\n</tr>\n<tr>\n<td>v1.0.3</td>\n<td>&gt;=3.15.2-01</td>\n</tr>\n<tr>\n<td>v1.0.4</td>\n<td>&gt;=3.18.0-01</td>\n</tr>\n</tbody>\n</table>\n\nplugin [Download link](https://github.com/sonatype-nexus-community/nexus-repository-r/releases)\n\n- nexus-repository-r-1.0.4.jar 기준으로 아래에 설명\n\n### 6-2. R-plugin 설치\n\n1.) 아래의 3개의 파일 복사와 설정 추가 필요함\n\n- `<nexus_dir>/system/org/sonatype/nexus/plugins/nexus-repository-r/1.0.4/nexus-repository-r-1.0.4.jar`\n- `<nexus_dir>/system/com/sonatype/nexus/assemblies/nexus-oss-feature/3.x.y/nexus-oss-feature-3.x.y-features.xml`\n- `<nexus_dir>/system/com/sonatype/nexus/assemblies/nexus-pro-feature/3.x.y/nexus-pro-feature-3.x.y-features.xml`\n\n```xml\n      <feature version=\"3.x.y.xy\" prerequisite=\"false\" dependency=\"false\">nexus-repository-rubygems</feature>\n+     <feature version=\"1.0.4\" prerequisite=\"false\" dependency=\"false\">nexus-repository-r</feature>\n      <feature version=\"3.x.y.xy\" prerequisite=\"false\" dependency=\"false\">nexus-repository-yum</feature>\n  </feature>\n```\n\n그리고,\n\n```xml\n+ <feature name=\"nexus-repository-r\" description=\"org.sonatype.nexus.plugins:nexus-repository-r\" version=\"1.0.3\">\n+     <details>org.sonatype.nexus.plugins:nexus-repository-r</details>\n+     <bundle>mvn:org.sonatype.nexus.plugins/nexus-repository-r/1.0.4</bundle>\n+ </feature>\n </features>\n```\n\n### 6-3. nexus repository 추가\n\n- 정상적으로 설치되면 nexus의 create repository 메뉴에 r(proxy), r(hosted), r(group) 메뉴가 표시됨\n- r(proxy),r(hosted) repository를 추가 후 r(group)으로 묶어서 repository 생성.\n  자세한 내용은 링크 참조\n  [nexus R repository 만들기](https://github.com/sonatype-nexus-community/nexus-repository-r/blob/master/docs/R_USER_DOCUMENTATION.md)\n\n### 6-4. 로컬 R 설정\n\n- R-cran을 proxy 를 거쳐가도록 repo list 설정이 필요\n\n```sh\n# OS 별 위치 상이 R home 명령어로 확인\n#‘/Library/Frameworks/R.framework/Resources/etc/’ on OS X,\n#‘C:Program FilesRR-***etc’ on Windows,\n#‘/etc/R/’ on Debian.\nR> R.home(component = \"home\") # 1st config. filename Rprofile.site\nR> path.expand(\"~\") # 2nd config. filename .Rprofile\n\n# edit $HOME/etc/\n## Default repo <— Rprofile.site\nlocal({r <- getOption(\"repos\")\n       r[\"Nexus\"] <- \"http://<nexusID>:<nexusPW>@<nexus r(group repository) address>\"\n       options(repos=r)\n})\n```\n\nR 신규 세션 연결 후 확인\n\n```R\nR> getOption('repos')\n----------------------------------------------------------------\n                                                    Nexus\n\"http://<nexusID>:<nexusPW>@<nexus r(group repository) address>\"\n----------------------------------------------------------------\nR>\n```\n\n### 6-5. R package download test\n\n```R\n> remove.packages('dplyr')\nRemoving package from ‘/Library/Frameworks/R.framework/Versions/3.5/Resources/library’\n(as ‘lib’ is unspecified)\nError in remove.packages : there is no package called ‘dplyr’\n> library(dplyr)\nError in library(dplyr) : there is no package called ‘dplyr’\n> getOption('repos')\n                                                    Nexus\n\"http://<nexusID>:<nexusPW>@<nexus r(group repository) address>\"\n> install.packages('dplyr')\ntrying URL 'http://<nexusID>:<nexusPW>@<nexus r(group repository) address>/bin/macosx/el-capitan/contrib/3.5/dplyr_0.8.3.tgz'\nContent type 'application/x-tgz' length 6265040 bytes (6.0 MB)\n==================================================\ndownloaded 6.0 MB\n\n\nThe downloaded binary packages are in\n\t/var/folders/98/5jh6pvh54_n1xqk_y7c66t580000gn/T//Rtmpxy9PGN/downloaded_packages\n> library(dplyr)\n\nAttaching package: ‘dplyr’\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\n\n>\n```\n\n### 6-6. R package upload test\n\n- upload는 curl 등으로 진행\n\n```sh\n## package의 DESCRIPTION version과 파일 명의 version 이 일치해야 함\ncurl -v --user '<nexusID>:<nexsusPW>' --upload-file userPackage.tar.gz http://localhost:8081/repository/r-snap/src/contrib/userPackage_0.1.0.tar.gz\n```\n\n```R\n##\n> install.packages('userPackage')\n오로지 소스형태로만 제공되는 패키지이므로 C/C++/Fortran으로 작성된\n  코드들에 대한 컴파일이 필요할 수도 있습니다.: ‘userPackage’\nDo you want to attempt to install these from sources? (Yes/no/cancel) Yes\n소스형태의 패키지 ‘userPackage’(들)를 설치합니다.\n\nURL 'http://<nexusID>:<nexusPW>@<nexus r(group repository) address>/src/contrib/userPackage_0.1.0.tar.gz'을 시도합니다\nContent type 'application/x-gzip' length 17215032 bytes (16.4 MB)\n==================================================\ndownloaded 16.4 MB\n\n* installing *source* package ‘userPackage’ ...\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded\n* DONE (userPackage)\n\n다운로드한 소스 패키지들은 다음의 위치에 있습니다\n        ‘/private/var/folders/98/5jh6pvh54_n1xqk_y7c66t580000gn/T/RtmppazSJ3/downloaded_packages’\n```\n\n혹은\n\n```r\ninstall.packages('userPackage',repos=\"http://<nexusID>:<nexusPW>@<nexus r(group repository) address>\");\n```\n\n### 6-7. 참고 사이트\n\n1. [r-plugin github site](https://github.com/sonatype-nexus-community/nexus-repository-r)\n2. [nexus 설정 방법](https://github.com/sonatype-nexus-community/nexus-repository-r/blob/master/docs/R_USER_DOCUMENTATION.md)\n3. [Rprofile 설정 방법](https://www.r-bloggers.com/fun-with-rprofile-and-customizing-r-startup/)"},{"id":"/2020/01/30/third-blog","metadata":{"permalink":"/2020/01/30/third-blog","source":"@site/blog/2020-01-30-third-blog.md","title":"Set up project dev environments","description":"summary","date":"2019-01-29T00:16:01.000Z","tags":[],"readingTime":8.615,"hasTruncateMarker":true,"authors":[{"name":"Hanbyul Cho","title":"Engineer","url":"https://github.com/hansgun","page":{"permalink":"/authors/hansgun"},"socials":{"linkedin":"https://www.linkedin.com/in/hanbyulcho1/","github":"https://github.com/hansgun"},"imageURL":"https://github.com/hansgun.png","key":"hansgun"}],"frontMatter":{"layout":"post","title":"Set up project dev environments","date":"2019-01-29 00:16:01 -0600","toc":true,"toc_label":"On This Page","toc_icon":"cog","toc_position":"sticky","authors":["hansgun"]},"unlisted":false,"prevItem":{"title":"Setup CI/CD [Jenkins+Nexus for Spring, Docker, R]","permalink":"/2019/11/17/first-blog"}},"content":"> summary\n<!-- truncate -->\n\n# 1. [Project] 개발 환경 설정\n\n---\n\n> 프로젝트 수행 결과로 민감한 정보는 [] 혹은 `<>` 으로 내용을 대치함\n\n---\n\n## 1.1 Host Naming Convention\n\n`[Project]-<flag><node-number>[-<env>].io`\n\n`<flag>`:\n\n- s: Service Node.\n- t: Streaming Node.\n- g: Gateway Node.\n\n`<node-number>`: Node Numbering 으로 예를들어, '01', '02',etc.\n\n`<env>`: Deploy 환경으로, 예를들어, 개발 환경일 경우, 'dev'. 상용 환경은 없음.\n\n## 1.2 Hosts File 등록\n\nLocal 개발 Machine 의 hosts file 에 다음과 같이 등록합니다:\n\n```bash\n<서버IP>    [Project]-s01-dev    [Project]-s01-dev.<full_name>\n```\n\n## 1.3 개발 Server 접속\n\n개발 Server 접속 방법은 다음과 같습니다:\n\n```bash\nssh -i <aws-pem> cento-s01-dev.io\n```\n\n접속후 사용자를 `<UserID>` 로 전환합니다:\n\n```bash\nsudo su - <UserID>;\n```\n\n## 1.4 설치된 Components\n\n개발 Server 에 설치된 중요 Component 들은 다음과 같습니다.\n\n- JDK 1.8\n- Maven 3.x\n- Docker\n- Redis\n- PostgreSQL\n\n## 1.5 PostgreSQL DB 연결\n\nPostgreSQL Client 로 PostgreSQL Server 에 다음과 같이 연결할수 있습니다.\n\nAirflow DB:\n\n```bash\npsql -h[Project]-s01-dev  -p 5432 -U airflow -W -d airflow;\n```\n\nUser sample DB:\n\n```bash\npsql -h[Project]-s01-dev -p 5432 -U <userschema> -W -d <userDB_name>;\n```\n```\n~~## 1.6 Airflow Components ( Old Version. See 1.7 section)~~\n\n~~설치된 Airflow Component 들은 Docker Container 로서 실행되고 있습니다.~~\n\n~~설치된 Airflow Component 들은 다음과 같습니다.~~\n\n~~WebServer: [http:<서버IP>:8080]~~\n~~Worker~~\n~~Scheduler~~\n~~Flower: [http:<서버IP>:5555]~~\n\n### ~~1.6.1 Airflow Docker 설치 방법 v.1 (Old Version.)~~\n\n~~Docker Container 형태로 Airflow 를 설치하는 방법을 설명하겠습니다.~~\n```\n\n```bash\n# add user to docker group.\nsudo su <userID>;\nsudo usermod -aG docker $(whoami);\n\n# airflow docker build.\n## NOTE: docker airflow is originated from https://github.com/puckel/docker-airflow\n\ngit clone https://github.com/[Project].git;\ncd [Project];\ngit fetch origin;\ngit checkout dev;\ncd workflow/docker-airflow;\n\n------ R 관련 추가 사항\n# R이 포함된 이미지는 workflow/docker-airflow/docker_with_R 에 있음..\n# customized R package tar 생성\ncd workflow/docker-airflow/docker_with_R\nsh make_skytale_tar.sh\ndocker build --rm -t <project_name>/docker-airflow-withR .\n# 아래의 docker build 생략\n------- R 관련 추가 사항(끝)\n\ndocker build --rm -t <project_name>/docker-airflow .\n\n# create directories for volumes:\nsudo mkdir -p /data01/airflow/docker-airflow-volumes;\nsudo mkdir -p /data01/airflow/docker-airflow-volumes/{logs,plugins,dags};\nsudo chmod 777 -R /data01/airflow/docker-airflow-volumes;\n\n# add python deps into requirements.txt.\nsudo vi /data01/airflow/docker-airflow-volumes/requirements.txt;\nFlask==1.0.4\npsycopg2-binary\n\n# install rest api plugin to the directory of plugins volume.\nwget https://github.com/teamclairvoyant/airflow-rest-api-plugin/archive/v1.0.5.tar.gz\n\n## copy all the files to '/data01/airflow/docker-airflow-volumes/plugins' dir.\n\n\nsudo su <userID>;\n## webserver, worker ::: -e LD_LIBRARY_PATH=/usr/lib/jvm/java-11-openjdk-amd64/lib/server/ \\ 추가\n# Dockerfile 자체에 JAVA_HOME 추가해서 하는 방법 확인 필요\n# run airflow webserver.\ndocker run -d \\\n    --add-host <서버이름>:<서버IP> \\\n    -v /data01/airflow/docker-airflow-volumes/plugins/:/usr/local/airflow/plugins \\\n    -v /data01/airflow/docker-airflow-volumes/dags/:/usr/local/airflow/dags \\\n    -v /data01/airflow/docker-airflow-volumes/logs/:/usr/local/airflow/logs \\\n    -v /data01/airflow/docker-airflow-volumes/requirements.txt:/requirements.txt \\\n    -v /data01/airflow/docker-airflow-volumes/temp/:/usr/local/airflow/temp \\\n    -e REDIS_HOST <서버IP> \\\n    -e POSTGRES_HOST <서버IP> \\\n    -e POSTGRES_USER=airflow \\\n    -e POSTGRES_PASSWORD=<password> \\\n    -e POSTGRES_DB=airflow \\\n    -e AIRFLOW__CORE__EXECUTOR=CeleryExecutor \\\n    -e AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://airflow:<password> <서버IP>:5432/airflow\" \\\n        -e AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://airflow:<password> <서버IP>:5432/airflow\" \\\n    -e AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False \\\n    -e AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul \\\n    -e LD_LIBRARY_PATH=/usr/lib/jvm/java-11-openjdk-amd64/lib/server/ \\\n    -v /etc/localtime:/etc/localtime:ro \\\n    -e TZ=Asia/Seoul \\\n    -p 8080:8080 \\\n    --name airflow-webserver \\\n    <project_name>/docker-airflow webserver\n\n\n# view docker logs: airflow webserver.\ndocker logs -f airflow-webserver;\n\n\n# run airflow worker.\ndocker run -d \\\n    --add-hosT <서버이름>:<서버IP> \\\n    -v /data01/airflow/docker-airflow-volumes/plugins/:/usr/local/airflow/plugins \\\n    -v /data01/airflow/docker-airflow-volumes/dags/:/usr/local/airflow/dags \\\n    -v /data01/airflow/docker-airflow-volumes/logs/:/usr/local/airflow/logs \\\n    -v /data01/airflow/docker-airflow-volumes/requirements.txt:/requirements.txt \\\n    -v /data01/airflow/docker-airflow-volumes/temp/:/usr/local/airflow/temp \\\n    -e REDIS_HOST <서버IP> \\\n    -e POSTGRES_HOST <서버IP> \\\n    -e POSTGRES_USER=airflow \\\n    -e POSTGRES_PASSWORD=<password> \\\n    -e POSTGRES_DB=airflow \\\n    -e AIRFLOW__CORE__EXECUTOR=CeleryExecutor \\\n    -e AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://airflow:<password> <서버IP>:5432/airflow\" \\\n        -e AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://airflow:<password> <서버IP>:5432/airflow\" \\\n    -e AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False \\\n    -e AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul \\\n    -e LD_LIBRARY_PATH=/usr/lib/jvm/java-11-openjdk-amd64/lib/server/ \\\n    -v /etc/localtime:/etc/localtime:ro \\\n    -e TZ=Asia/Seoul \\\n    -p 8793:8793 \\\n    --name airflow-worker \\\n    <project_name>/docker-airflow worker\n\n\n# view docker logs: airflow worker.\ndocker logs -f airflow-worker;\n\n\n# run airflow scheduler.\ndocker run -d \\\n    --add-host <서버이름>:<서버IP> \\\n    -v /data01/airflow/docker-airflow-volumes/plugins/:/usr/local/airflow/plugins \\\n    -v /data01/airflow/docker-airflow-volumes/dags/:/usr/local/airflow/dags \\\n    -v /data01/airflow/docker-airflow-volumes/logs/:/usr/local/airflow/logs \\\n    -v /data01/airflow/docker-airflow-volumes/requirements.txt:/requirements.txt \\\n    -v /data01/airflow/docker-airflow-volumes/temp/:/usr/local/airflow/temp \\\n    -e REDIS_HOST=<서버IP> \\\n    -e POSTGRES_HOST=<서버IP> \\\n    -e POSTGRES_USER=airflow \\\n    -e POSTGRES_PASSWORD=<password> \\\n    -e POSTGRES_DB=airflow \\\n        -e AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=2 \\\n    -e AIRFLOW__CORE__EXECUTOR=CeleryExecutor \\\n    -e AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://airflow:<password>@<서버IP>:5432/airflow\" \\\n        -e AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://airflow:<password>@<서버IP>:5432/airflow\" \\\n    -e AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False \\\n    -e AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul \\\n    -v /etc/localtime:/etc/localtime:ro \\\n    -e TZ=Asia/Seoul \\\n    --name airflow-scheduler \\\n    <project_name>/docker-airflow scheduler\n\n\n# view docker logs: airflow scheduler.\ndocker logs -f airflow-scheduler;\n\n\n# run airflow flower.\ndocker run -d \\\n    --add-host <서버이름>:<서버IP> \\\n    -v /data01/airflow/docker-airflow-volumes/plugins/:/usr/local/airflow/plugins \\\n    -v /data01/airflow/docker-airflow-volumes/dags/:/usr/local/airflow/dags \\\n    -v /data01/airflow/docker-airflow-volumes/logs/:/usr/local/airflow/logs \\\n    -v /data01/airflow/docker-airflow-volumes/requirements.txt:/requirements.txt \\\n    -v /data01/airflow/docker-airflow-volumes/temp/:/usr/local/airflow/temp \\\n    -e REDIS_HOST=<서버IP> \\\n    -e POSTGRES_HOST=<서버IP> \\\n    -e POSTGRES_USER=airflow \\\n    -e POSTGRES_PASSWORD=<password> \\\n    -e POSTGRES_DB=airflow \\\n    -e AIRFLOW__CORE__EXECUTOR=CeleryExecutor \\\n    -e AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://airflow:<password>@<서버IP>:5432/airflow\" \\\n        -e AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://airflow:<password>@<서버IP>:5432/airflow\" \\\n    -e AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False \\\n    -e AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul \\\n    -v /etc/localtime:/etc/localtime:ro \\\n    -e TZ=Asia/Seoul \\\n    -p 5555:5555 \\\n    --name airflow-flower \\\n    <project_name>/docker-airflow flower\n\n\n# view docker logs: airflow flower.\ndocker logs -f airflow-flower;\n\n\n# check rest api.\ncurl --header \"rest_api_plugin_http_token: changeme\" http://<서버IP>:8080/admin/rest_api/api?api=version\n\n# deploy dag.\ncurl -X POST --header \"rest_api_plugin_http_token: changeme\" -H 'Content-Type: multipart/form-data' -F 'dag_file=@<project_name>-test-dag8.py' -F 'force=on' http://<서버IP>:8080/admin/rest_api/api?api=deploy_dag\n\n# trigger dag.\ncurl --header \"rest_api_plugin_http_token: changeme\" http://<서버IP>:8080/admin/rest_api/api?api=trigger_dag&dag_id=tutorial-<project_name>7\n```\n\n\n## 1.7 Airflow Docker 설치 방법 v.2 (using docker-compose)\n\n- docker compose를 통하여 component dependency를 고려하여 single command로 실행\n- 실제 수행 내용은 v.1 과 동일.\n\n### 1.7.1 설치과정\n\n파일 위치 : `[Project]/workflow/docker-airflow` \n관련 파일 :\n\n- `docker-compose.yml`\n- `.env` [ .env는 현재 local 설정, 개발기에서는 .env_dev와 교체 필요.. ]\n- 실행 명령 : `docker-compose -p [Project] up -d --build` 실행 시 container이름은 `[Project]_<container 이름>_<인스턴스 번호>` 형태\n\n`.env` 파일 내용 : 환경에 맞게 변경 필요\n\n```\n###### section 1. postgre\nPOSTGRES_HOST=192.168.3.108\necho ${POSTGRES_HOST}\nPOSTGRES_USER=airflow\nPOSTGRES_PASSWORD=***** # 실제 패스워드\n\n###### section 2. redis\nREDIS_HOST=192.168.3.108\n\n###### section 3. APP\nAPP_HOST_NAME=[Project]-app.name\nAPP_HOST_IP=192.168.3.108\nAPP_VOLUMN_DIR=/Users/han/data01/airflow/docker-airflowvolumns\n\n```\n\n`docker-compose.yml` 파일 내용 (주석 내용은 현재 환경에 이미 설치되어 있다고 가정하였을 때)\n\n```yaml\nversion: '2.1'\n\n services:\n #  postgre:\n #    image: postgres:10.10\n #    environment:\n #      - POSTGRES_DB=airflow\n #      - POSTGRES_USER=${POSTGRES_USER}\n #      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n #      - POSTGRES_INITDB_ARGS=--encoding=UTF-8\n #    ports:\n #      - \"5432:5432\"\n #    healthcheck:\n #      test: \"pg_isready -h localhost -p 5432 -q -U postgres\"\n #      interval: 3s\n #      timeout: 1s\n #      retries: 10\n #\n #  redis:\n #    image: redis:3.2.12\n #    ports:\n #      - \"6379:6379\"\n   airflow-webserver:\n     build:\n       context: .\n     command: webserver\n     environment:\n       - REDIS_HOST=${APP_HOST_NAME}\n       - POSTGRES_HOST=${POSTGRES_HOST}\n       - POSTGRES_USER=${POSTGRES_USER}\n       - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n       - POSTGRES_DB=airflow\n       - AIRFLOW__CORE__EXECUTOR=CeleryExecutor\n       - AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n       - AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul\n       - TZ=Asia/Seoul\n     extra_hosts:\n       - ${APP_HOST_NAME}:${APP_HOST_IP}\n     volumes:\n       - ${APP_VOLUMN_DIR}/plugins/:/usr/local/airflow/plugins\n       - ${APP_VOLUMN_DIR}/dags/:/usr/local/airflow/dags\n       - ${APP_VOLUMN_DIR}/logs/:/usr/local/airflow/logs\n         #- ${APP_VOLUMN_DIR}/requirements.txt:/requirements.txt\n       - ${APP_VOLUMN_DIR}/temp/:/usr/local/airflow/temp\n       - /etc/localtime:/etc/localtime:ro\n     ports:\n       - \"8080:8080\"\n         #depends_on:\n         #- postgre\n         #- redis\n\n   airflow-scheduler:\n     build:\n       context: .\n     command: scheduler\n     environment:\n       - REDIS_HOST=${APP_HOST_NAME}\n       - POSTGRES_HOST=${POSTGRES_HOST}\n       - POSTGRES_USER=${POSTGRES_USER}\n       - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n       - POSTGRES_DB=airflow\n       - AIRFLOW__CORE__EXECUTOR=CeleryExecutor\n       - AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n       - AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul\n       - TZ=Asia/Seoul\n     extra_hosts:\n       - ${APP_HOST_NAME}:${APP_HOST_IP}\n     volumes:\n       - ${APP_VOLUMN_DIR}/plugins/:/usr/local/airflow/plugins\n       - ${APP_VOLUMN_DIR}/dags/:/usr/local/airflow/dags\n       - ${APP_VOLUMN_DIR}/logs/:/usr/local/airflow/logs\n         #- ${APP_VOLUMN_DIR}/requirements.txt:/requirements.txt\n       - ${APP_VOLUMN_DIR}/temp/:/usr/local/airflow/temp\n       - /etc/localtime:/etc/localtime:ro\n     depends_on:\n       - airflow-webserver\n\n\n   airflow-flower:\n     build:\n       context: .\n     command: flower\n     environment:\n       - REDIS_HOST=${APP_HOST_NAME}\n       - POSTGRES_HOST=${POSTGRES_HOST}\n       - POSTGRES_USER=${POSTGRES_USER}\n       - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n       - POSTGRES_DB=airflow\n       - AIRFLOW__CORE__EXECUTOR=CeleryExecutor\n       - AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n       - AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul\n       - TZ=Asia/Seoul\n     extra_hosts:\n       - ${APP_HOST_NAME}:${APP_HOST_IP}\n     volumes:\n       - ${APP_VOLUMN_DIR}/plugins/:/usr/local/airflow/plugins\n       - ${APP_VOLUMN_DIR}/dags/:/usr/local/airflow/dags\n       - ${APP_VOLUMN_DIR}/logs/:/usr/local/airflow/logs\n         #- ${APP_VOLUMN_DIR}/requirements.txt:/requirements.txt\n       - ${APP_VOLUMN_DIR}/temp/:/usr/local/airflow/temp\n       - /etc/localtime:/etc/localtime:ro\n     ports:\n       - \"5555:5555\"\n         #depends_on:\n         #- redis\n\n   airflow-worker:\n     build:\n       context: ./docker_with_R/\n     command: worker\n     environment:\n       - REDIS_HOST=${APP_HOST_NAME}\n       - POSTGRES_HOST=${POSTGRES_HOST}\n       - POSTGRES_USER=${POSTGRES_USER}\n       - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n       - POSTGRES_DB=airflow\n       - AIRFLOW__CORE__EXECUTOR=CeleryExecutor\n       - AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CELERY__RESULT_BACKEND=\"db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/airflow\"\n       - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n       - AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Seoul\n       - TZ=Asia/Seoul\n     extra_hosts:\n       - ${APP_HOST_NAME}:${APP_HOST_IP}\n     volumes:\n       - ${APP_VOLUMN_DIR}/plugins/:/usr/local/airflow/plugins\n       - ${APP_VOLUMN_DIR}/dags/:/usr/local/airflow/dags\n       - ${APP_VOLUMN_DIR}/logs/:/usr/local/airflow/logs\n         #- ${APP_VOLUMN_DIR}/requirements.txt:/requirements.txt\n       - ${APP_VOLUMN_DIR}/temp/:/usr/local/airflow/temp\n       - /etc/localtime:/etc/localtime:ro\n     ports:\n       - \"8793:8793\"\n     depends_on:\n       - airflow-scheduler\n```\n\n\n### 1.7.2 실행결과\n\n```bash\n> docker-compose -p [Project] up -d\nRemoving [Project]_airflow-webserver_1\nRecreating [Project]_airflow-flower_1                 ... done\nRecreating b415085d2612_[Project]_airflow-webserver_1 ... done\nCreating [Project]_airflow-scheduler_1                ... done\nCreating [Project]_airflow-worker_1                   ... done\n> docker ps\nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n3a82d6c17ee0        [Project]_airflow-worker      \"/entrypoint.sh work…\"   5 seconds ago       Up 4 seconds        5555/tcp, 8080/tcp, 0.0.0.0:8793->8793/tcp       [Project]_airflow-worker_1\n87e87e660969        [Project]_airflow-scheduler   \"/entrypoint.sh sche…\"   6 seconds ago       Up 5 seconds        5555/tcp, 8080/tcp, 8793/tcp                     [Project]_airflow-scheduler_1\nc70230f8b1d3        [Project]_airflow-flower      \"/entrypoint.sh flow…\"   7 seconds ago       Up 6 seconds        8080/tcp, 0.0.0.0:5555->5555/tcp, 8793/tcp       [Project]_airflow-flower_1\nc726e9fdfbc2        [Project]_airflow-webserver   \"/entrypoint.sh webs…\"   7 seconds ago       Up 6 seconds        5555/tcp, 8793/tcp, 0.0.0.0:8080->8080/tcp       [Project]_airflow-webserver_1\n>\n\n```\n\n### 1.7.3 scale-out test\n\n```bash\n> docker ps                                                                                                                        CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                        NAMES\n32b99476f0ab        airflow_worker      \"/entrypoint.sh work…\"   26 seconds ago      Up 26 seconds       5555/tcp, 8080/tcp, 8793/tcp                 airflow_worker_1\nac788e72dd21        airflow_scheduler   \"/entrypoint.sh sche…\"   27 seconds ago      Up 27 seconds       5555/tcp, 8080/tcp, 8793/tcp                 airflow_scheduler_1\n0cb6456b4869        airflow_flower      \"/entrypoint.sh flow…\"   29 seconds ago      Up 27 seconds       8080/tcp, 0.0.0.0:5555->5555/tcp, 8793/tcp   airflow_flower_1\n4483b48d0078        airflow_webserver   \"/entrypoint.sh webs…\"   29 seconds ago      Up 28 seconds       5555/tcp, 8793/tcp, 0.0.0.0:8080->8080/tcp   airflow_webserver_1\n> docker-compose -p airflow scale worker=2\nWARNING: The scale command is deprecated. Use the up command with the --scale flag instead.\nStarting airflow_worker_1 ... done\nCreating airflow_worker_2 ... done\n> docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                        NAMES\n71e9fce448f2        airflow_worker      \"/entrypoint.sh work…\"   4 seconds ago       Up 2 seconds        5555/tcp, 8080/tcp, 8793/tcp                 airflow_worker_2\n32b99476f0ab        airflow_worker      \"/entrypoint.sh work…\"   44 seconds ago      Up 43 seconds       5555/tcp, 8080/tcp, 8793/tcp                 airflow_worker_1\nac788e72dd21        airflow_scheduler   \"/entrypoint.sh sche…\"   45 seconds ago      Up 44 seconds       5555/tcp, 8080/tcp, 8793/tcp                 airflow_scheduler_1\n0cb6456b4869        airflow_flower      \"/entrypoint.sh flow…\"   47 seconds ago      Up 45 seconds       8080/tcp, 0.0.0.0:5555->5555/tcp, 8793/tcp   airflow_flower_1\n4483b48d0078        airflow_webserver   \"/entrypoint.sh webs…\"   47 seconds ago      Up 45 seconds       5555/tcp, 8793/tcp, 0.0.0.0:8080->8080/tcp   airflow_webserver_1\n>\n```\n\n## 1.8 R Package 설치 방법\n\n```bash\ncd ${[Project]_ROOT_DIR}/workflow/job-runner/skytale\n\nR -e \"install.packages('devtools', repos='https://cloud.r-project.org'); library(devtools); devtools::install('./')\"\n```"}],"blogListPaginated":[{"items":["/helllo_world","/2024/12/18/forth-prometheus","/2024/11/01/fifth-Redis-cluster","/2024/10/26/sixth-terrform-aks","/2020/01/29/second-blog","/2019/11/17/first-blog","/2020/01/30/third-blog"],"metadata":{"permalink":"/","page":1,"postsPerPage":10,"totalPages":1,"totalCount":7,"blogDescription":"Blog","blogTitle":"Blog"}}],"blogTags":{"/tags/Observability":{"inline":false,"label":"Observability","permalink":"/tags/Observability","description":"Observability tag description","items":["/2024/12/18/forth-prometheus"],"pages":[{"items":["/2024/12/18/forth-prometheus"],"metadata":{"permalink":"/tags/Observability","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/Prometheus":{"inline":false,"label":"Prometheus","permalink":"/tags/Prometheus","description":"Prometheus tag description","items":["/2024/12/18/forth-prometheus"],"pages":[{"items":["/2024/12/18/forth-prometheus"],"metadata":{"permalink":"/tags/Prometheus","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/Loki":{"inline":false,"label":"Loki","permalink":"/tags/Loki","description":"Loki tag description","items":["/2024/12/18/forth-prometheus"],"pages":[{"items":["/2024/12/18/forth-prometheus"],"metadata":{"permalink":"/tags/Loki","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/Tempo":{"inline":false,"label":"Tempo","permalink":"/tags/Tempo","description":"Tempo tag description","items":["/2024/12/18/forth-prometheus"],"pages":[{"items":["/2024/12/18/forth-prometheus"],"metadata":{"permalink":"/tags/Tempo","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/Opentelemetry":{"inline":false,"label":"Opentelemetry","permalink":"/tags/Opentelemetry","description":"Opentelemetry tag description","items":["/2024/12/18/forth-prometheus"],"pages":[{"items":["/2024/12/18/forth-prometheus"],"metadata":{"permalink":"/tags/Opentelemetry","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/auto-instrument":{"inline":false,"label":"auto-instrument","permalink":"/tags/auto-instrument","description":"auto-instrument tag description","items":["/2024/12/18/forth-prometheus"],"pages":[{"items":["/2024/12/18/forth-prometheus"],"metadata":{"permalink":"/tags/auto-instrument","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/Redis Cluster":{"inline":false,"label":"Redis Cluster","permalink":"/tags/Redis Cluster","description":"Redis Cluster tag description","items":["/2024/11/01/fifth-Redis-cluster"],"pages":[{"items":["/2024/11/01/fifth-Redis-cluster"],"metadata":{"permalink":"/tags/Redis Cluster","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/kubernetes":{"inline":false,"label":"kubernetes","permalink":"/tags/kubernetes","description":"kubernetes tag description","items":["/2024/11/01/fifth-Redis-cluster","/2024/10/26/sixth-terrform-aks"],"pages":[{"items":["/2024/11/01/fifth-Redis-cluster","/2024/10/26/sixth-terrform-aks"],"metadata":{"permalink":"/tags/kubernetes","page":1,"postsPerPage":10,"totalPages":1,"totalCount":2,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/GKE":{"inline":false,"label":"GKE","permalink":"/tags/GKE","description":"GKE tag description","items":["/2024/10/26/sixth-terrform-aks"],"pages":[{"items":["/2024/10/26/sixth-terrform-aks"],"metadata":{"permalink":"/tags/GKE","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/docker-compose":{"inline":false,"label":"docker-compose","permalink":"/tags/docker-compose","description":"docker-compose tag description","items":["/2020/01/29/second-blog"],"pages":[{"items":["/2020/01/29/second-blog"],"metadata":{"permalink":"/tags/docker-compose","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/authorizer":{"inline":false,"label":"authorizer","permalink":"/tags/authorizer","description":"authorizer tag description","items":["/2020/01/29/second-blog"],"pages":[{"items":["/2020/01/29/second-blog"],"metadata":{"permalink":"/tags/authorizer","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/jenkins":{"inline":false,"label":"jenkins","permalink":"/tags/jenkins","description":"jenkins tag description","items":["/2019/11/17/first-blog"],"pages":[{"items":["/2019/11/17/first-blog"],"metadata":{"permalink":"/tags/jenkins","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/nexus OSS":{"inline":false,"label":"nexus OSS","permalink":"/tags/nexus OSS","description":"nexus OSS tag description","items":["/2019/11/17/first-blog"],"pages":[{"items":["/2019/11/17/first-blog"],"metadata":{"permalink":"/tags/nexus OSS","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false},"/tags/CI/CD":{"inline":false,"label":"CI/CD","permalink":"/tags/CI/CD","description":"CI/CD tag description","items":["/2019/11/17/first-blog"],"pages":[{"items":["/2019/11/17/first-blog"],"metadata":{"permalink":"/tags/CI/CD","page":1,"postsPerPage":10,"totalPages":1,"totalCount":1,"blogDescription":"Blog","blogTitle":"Blog"}}],"unlisted":false}},"blogTagsListPath":"/tags","authorsMap":{"hansgun":{"name":"Hanbyul Cho","title":"Engineer","url":"https://github.com/hansgun","page":{"permalink":"/authors/hansgun"},"socials":{"linkedin":"https://www.linkedin.com/in/hanbyulcho1/","github":"https://github.com/hansgun"},"imageURL":"https://github.com/hansgun.png","key":"hansgun"}}}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.tsx"},{"type":"mdx","permalink":"/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false}]},"docusaurus-plugin-debug":{},"docusaurus-theme-classic":{},"docusaurus-plugin-zooming":{},"docusaurus-theme-mermaid":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}